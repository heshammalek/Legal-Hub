import logging
from typing import Dict, Any, Optional, AsyncGenerator
import os
import asyncio

# Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ù…Ø¯ÙŠØ± Ø§Ù„ÙƒØ§Ø´
from .cache_manager import CacheManager

logger = logging.getLogger(__name__)

class MultiLLMOrchestrator:
    """
    Ù…Ù†Ø³Ù‚ Ù…Ø­Ø³Ù‘Ù† Ù…Ø¹ Ø¯Ø¹Ù… AWS Bedrock ÙˆÙ…Ø¹Ø§Ù„Ø¬Ø© Ø£Ø®Ø·Ø§Ø¡ Ø§Ù„Ø§Ø³ØªÙŠØ±Ø§Ø¯
    """
    
    def __init__(self, config: Dict[str, Any], cache_manager: Optional[CacheManager] = None):
        self.models: Dict[str, Any] = {}
        self.cache_manager = cache_manager
        self._initialize_models(config.get("models", {}))

    def _initialize_models(self, models_config: Dict[str, Any]):
        """ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ù…Ø¹ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø£Ø®Ø·Ø§Ø¡ Ø§Ù„Ø§Ø³ØªÙŠØ±Ø§Ø¯"""
        
        # ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…ÙØ§ØªÙŠØ­ Ù…Ù† Ù…ØªØºÙŠØ±Ø§Øª Ø§Ù„Ø¨ÙŠØ¦Ø©
        api_keys = {
            "google": os.environ.get("GOOGLE_API_KEY"),
            "openai": os.environ.get("OPENAI_API_KEY"),
            "anthropic": os.environ.get("ANTHROPIC_API_KEY"),
            "aws": os.environ.get("AWS_ACCESS_KEY_ID")  # Ù„Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ÙˆØ¬ÙˆØ¯ AWS
        }
        
        for key, conf in models_config.items():
            provider = conf.get("provider")
            model_name = conf.get("model_name")
            
            try:
                model = self._create_model(provider, model_name, api_keys)
                if model:
                    self.models[key] = model
                    logger.info(f"âœ… ØªÙ… ØªÙ‡ÙŠØ¦Ø© Ù†Ù…ÙˆØ°Ø¬ '{key}' ({provider} - {model_name})")
                else:
                    logger.warning(f"âš ï¸ Ù„Ù… ÙŠØªÙ… ØªÙ‡ÙŠØ¦Ø© Ù†Ù…ÙˆØ°Ø¬ '{key}' - Ø§Ù„Ù…Ø²ÙˆØ¯ ØºÙŠØ± Ù…Ø¯Ø¹ÙˆÙ…: {provider}")
                    
            except Exception as e:
                logger.error(f"âŒ ÙØ´Ù„ ØªÙ‡ÙŠØ¦Ø© Ù†Ù…ÙˆØ°Ø¬ '{key}'. Ø®Ø·Ø£: {e}")

    def _create_model(self, provider: str, model_name: str, api_keys: Dict) -> Optional[Any]:
        """Ø¥Ù†Ø´Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ Ù…Ø¹ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø£Ø®Ø·Ø§Ø¡ Ø§Ù„Ø§Ø³ØªÙŠØ±Ø§Ø¯"""
        try:
            if provider == "google":
                return self._create_google_model(model_name, api_keys["google"])
            
            elif provider == "openai":
                return self._create_openai_model(model_name, api_keys["openai"])
            
            elif provider == "anthropic":
                return self._create_anthropic_model(model_name, api_keys["anthropic"])
            
            elif provider == "aws_bedrock":
                return self._create_bedrock_model(model_name, api_keys["aws"])
            
            else:
                logger.warning(f"âš ï¸ Ù…Ø²ÙˆØ¯ ØºÙŠØ± Ù…Ø¹Ø±ÙˆÙ: {provider}")
                return None
                
        except Exception as e:
            logger.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ {provider}: {e}")
            return None

    def _create_google_model(self, model_name: str, api_key: str) -> Optional[Any]:
        """Ø¥Ù†Ø´Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ Google Ù…Ø¹ Ø¨Ø¯Ø§Ø¦Ù„"""
        if not api_key:
            raise ValueError("GOOGLE_API_KEY ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯")
        
        try:
            # Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø© Ø§Ù„Ø£ÙˆÙ„Ù‰: langchain_google_genai
            from langchain_google_genai import ChatGoogleGenerativeAI
            return ChatGoogleGenerativeAI(
                model=model_name, temperature=0.1, google_api_key=api_key
            )
        except ImportError:
            try:
                # Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø© Ø§Ù„Ø«Ø§Ù†ÙŠØ©: langchain_community
                logger.warning("âš ï¸ langchain_google_genai ØºÙŠØ± Ù…Ø«Ø¨Øª - Ø§Ø³ØªØ®Ø¯Ø§Ù… langchain_community")
                from langchain_community.chat_models import ChatGoogleGenerativeAI
                return ChatGoogleGenerativeAI(
                    model=model_name, temperature=0.1, google_api_key=api_key
                )
            except ImportError:
                logger.error("âŒ ÙØ´Ù„ ØªØ­Ù…ÙŠÙ„ ChatGoogleGenerativeAI Ù…Ù† Ø£ÙŠ Ù…ØµØ¯Ø±")
                return None

    def _create_openai_model(self, model_name: str, api_key: str) -> Optional[Any]:
        """Ø¥Ù†Ø´Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ OpenAI Ù…Ø¹ Ø¨Ø¯Ø§Ø¦Ù„"""
        if not api_key:
            raise ValueError("OPENAI_API_KEY ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯")
        
        try:
            # Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø© Ø§Ù„Ø£ÙˆÙ„Ù‰: langchain_openai
            from langchain_openai import ChatOpenAI
            return ChatOpenAI(
                model=model_name, temperature=0.1, api_key=api_key
            )
        except ImportError:
            try:
                # Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø© Ø§Ù„Ø«Ø§Ù†ÙŠØ©: langchain_community
                logger.warning("âš ï¸ langchain_openai ØºÙŠØ± Ù…Ø«Ø¨Øª - Ø§Ø³ØªØ®Ø¯Ø§Ù… langchain_community")
                from langchain_community.chat_models import ChatOpenAI
                return ChatOpenAI(
                    model_name=model_name, temperature=0.1, openai_api_key=api_key
                )
            except ImportError:
                logger.error("âŒ ÙØ´Ù„ ØªØ­Ù…ÙŠÙ„ ChatOpenAI Ù…Ù† Ø£ÙŠ Ù…ØµØ¯Ø±")
                return None

    def _create_anthropic_model(self, model_name: str, api_key: str) -> Optional[Any]:
        """Ø¥Ù†Ø´Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ Anthropic Ù…Ø¹ Ø¨Ø¯Ø§Ø¦Ù„"""
        if not api_key:
            raise ValueError("ANTHROPIC_API_KEY ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯")
        
        try:
            # Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø© Ø§Ù„Ø£ÙˆÙ„Ù‰: langchain_anthropic
            from langchain_anthropic import ChatAnthropic
            return ChatAnthropic(
                model=model_name, temperature=0.1, api_key=api_key
            )
        except ImportError:
            try:
                # Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø© Ø§Ù„Ø«Ø§Ù†ÙŠØ©: langchain_community
                logger.warning("âš ï¸ langchain_anthropic ØºÙŠØ± Ù…Ø«Ø¨Øª - Ø§Ø³ØªØ®Ø¯Ø§Ù… langchain_community")
                from langchain_community.chat_models import ChatAnthropic
                return ChatAnthropic(
                    model=model_name, temperature=0.1, anthropic_api_key=api_key
                )
            except ImportError:
                logger.error("âŒ ÙØ´Ù„ ØªØ­Ù…ÙŠÙ„ ChatAnthropic Ù…Ù† Ø£ÙŠ Ù…ØµØ¯Ø±")
                return None

    def _create_bedrock_model(self, model_name: str, aws_key: str) -> Optional[Any]:
        """Ø¥Ù†Ø´Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ AWS Bedrock"""
        if not aws_key:
            logger.warning("âš ï¸ Ù…ÙØ§ØªÙŠØ­ AWS ØºÙŠØ± Ù…ØªÙˆÙØ±Ø© - ØªØ®Ø·ÙŠ Bedrock")
            return None
        
        try:
            # Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø© Ø§Ù„Ø£ÙˆÙ„Ù‰: langchain_aws
            try:
                from langchain_aws import ChatBedrock
                import boto3
                
                bedrock_client = boto3.client('bedrock-runtime', region_name=os.getenv('AWS_REGION', 'us-east-1'))
                return ChatBedrock(
                    model_id=model_name,
                    client=bedrock_client,
                    model_kwargs={"temperature": 0.1, "max_tokens": 4096}
                )
            except ImportError:
                # Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø© Ø§Ù„Ø«Ø§Ù†ÙŠØ©: Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„ØªÙ†ÙÙŠØ° Ø§Ù„Ù…Ø¨Ø§Ø´Ø±
                logger.warning("âš ï¸ langchain_aws ØºÙŠØ± Ù…Ø«Ø¨Øª - Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„ØªÙ†ÙÙŠØ° Ø§Ù„Ù…Ø¨Ø§Ø´Ø±")
                from ..aws_services.bedrock_llm import AWSBedrockLLM
                bedrock_llm = AWSBedrockLLM()
                return bedrock_llm.get_model('claude_haiku')  # Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù†Ù…ÙˆØ°Ø¬ Ø§ÙØªØ±Ø§Ø¶ÙŠ
                
        except Exception as e:
            logger.error(f"âŒ ÙØ´Ù„ Ø¥Ù†Ø´Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ Bedrock: {e}")
            return None

    def get_model(self, model_key: str = "fast") -> Any:
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ù†Ù…ÙˆØ°Ø¬ Ù…Ø¹ Ø§Ù„ØªØ¹Ø§Ù…Ù„ Ù…Ø¹ Ø§Ù„Ø£Ø®Ø·Ø§Ø¡"""
        model = self.models.get(model_key)
        if not model:
            logger.warning(f"âš ï¸ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ '{model_key}' ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯. Ø¬Ø§Ø±ÙŠ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø¯ÙŠÙ„.")
            
            # Ù…Ø­Ø§ÙˆÙ„Ø© Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø£ÙŠ Ù†Ù…ÙˆØ°Ø¬ Ù…ØªØ§Ø­
            for key, available_model in self.models.items():
                if available_model:
                    logger.info(f"ðŸ” Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ '{key}' ÙƒØ¨Ø¯ÙŠÙ„")
                    return available_model
            
            # Ø¥Ø°Ø§ Ù„Ù… ÙŠÙˆØ¬Ø¯ Ø£ÙŠ Ù†Ù…ÙˆØ°Ø¬ØŒ Ø±ÙØ¹ Ø®Ø·Ø£
            raise RuntimeError("âŒ Ù„Ø§ ØªÙˆØ¬Ø¯ Ù†Ù…Ø§Ø°Ø¬ LLM Ù…ØªØ§Ø­Ø©. ØªØ£ÙƒØ¯ Ù…Ù† ØªÙƒÙˆÙŠÙ† Ø§Ù„Ù…ÙØ§ØªÙŠØ­ ÙˆØ§Ù„Ù…ÙƒØªØ¨Ø§Øª.")
        
        return model

    async def generate_response_stream(
        self, 
        system_prompt: str, 
        human_prompt: str, 
        context: Optional[str] = None, 
        model_key: str = "fast"
    ) -> AsyncGenerator[str, None]:
        """
        Ø¥Ù†Ø´Ø§Ø¡ Ø±Ø¯ Ù…ØªØ¯ÙÙ‚ Ù…Ø¹ Ø§Ù„ØªØ¹Ø§Ù…Ù„ Ù…Ø¹ Ø§Ù„Ø£Ø®Ø·Ø§Ø¡
        """
        try:
            model = self.get_model(model_key)
            
            if context:
                full_system_prompt = f"{system_prompt}\n\nØ§Ù„Ø³ÙŠØ§Ù‚:\n{context}"
            else:
                full_system_prompt = system_prompt
            
            # Ø§Ø³ØªØ®Ø¯Ø§Ù… LangChain Ù„Ù„Ø¨Ø«
            from langchain_core.prompts import ChatPromptTemplate
            from langchain_core.output_parsers import StrOutputParser
            
            prompt_template = ChatPromptTemplate.from_messages([
                ("system", full_system_prompt),
                ("human", "{input}")
            ])
            
            chain = prompt_template | model | StrOutputParser()
            
            logger.debug(f"Ø¨Ø¯Ø¡ Ø¨Ø« Ø§Ù„Ø§Ø³ØªØ¬Ø§Ø¨Ø© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù†Ù…ÙˆØ°Ø¬ '{model_key}'...")
            
            # Ø§Ù„Ø¨Ø« Ø§Ù„Ù…ØªØ¯ÙÙ‚
            async for chunk in chain.astream({"input": human_prompt}):
                yield chunk
                await asyncio.sleep(0)  # Ù„Ù„Ø³Ù…Ø§Ø­ Ø¨Ù…Ù‡Ø§Ù… Ø£Ø®Ø±Ù‰

        except Exception as e:
            logger.error(f"âŒ Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ Ø¨Ø« Ø§Ù„Ø§Ø³ØªØ¬Ø§Ø¨Ø©: {e}")
            yield f"\n\n[Ø­Ø¯Ø« Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ù†Ø¸Ø§Ù…: {e}]"

    async def generate_response(
        self, 
        system_prompt: str, 
        human_prompt: str, 
        context: Optional[str] = None, 
        model_key: str = "fast",
        use_cache: bool = True
    ) -> str:
        """
        Ø¥Ù†Ø´Ø§Ø¡ Ø±Ø¯ ÙƒØ§Ù…Ù„ Ù…Ø¹ Ø¯Ø¹Ù… Ø§Ù„ÙƒØ§Ø´
        """
        
        # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„ÙƒØ§Ø´
        cache_key = ""
        if self.cache_manager and use_cache:
            cache_key = f"llm_response:{model_key}:{system_prompt}:{human_prompt}:{context}"
            cached_response = await self.cache_manager.get(cache_key)
            if cached_response:
                logger.debug("LLM response (non-stream) HIT from cache.")
                return cached_response

        try:
            model = self.get_model(model_key)
            
            if context:
                full_system_prompt = f"{system_prompt}\n\nØ§Ù„Ø³ÙŠØ§Ù‚:\n{context}"
            else:
                full_system_prompt = system_prompt
                
            from langchain_core.prompts import ChatPromptTemplate
            from langchain_core.output_parsers import StrOutputParser
            
            prompt_template = ChatPromptTemplate.from_messages([
                ("system", full_system_prompt),
                ("human", "{input}")
            ])
            
            chain = prompt_template | model | StrOutputParser()
            
            logger.debug(f"Ø¥Ù†Ø´Ø§Ø¡ Ø§Ø³ØªØ¬Ø§Ø¨Ø© ÙƒØ§Ù…Ù„Ø© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù†Ù…ÙˆØ°Ø¬ '{model_key}'...")
            response = await chain.ainvoke({"input": human_prompt})

            # ØªØ®Ø²ÙŠÙ† ÙÙŠ Ø§Ù„ÙƒØ§Ø´
            if self.cache_manager and use_cache:
                await self.cache_manager.set(cache_key, response, ttl=7200)

            return response
            
        except Exception as e:
            logger.error(f"âŒ Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ø§Ø³ØªØ¬Ø§Ø¨Ø©: {e}")
            return f"[Ø­Ø¯Ø« Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ù†Ø¸Ø§Ù…: {e}]"

    def get_available_models(self) -> Dict[str, str]:
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ù…ØªØ§Ø­Ø©"""
        available = {}
        for key, model in self.models.items():
            if model:
                model_type = type(model).__name__
                available[key] = f"{model_type} (Active)"
            else:
                available[key] = "Not Available"
        return available