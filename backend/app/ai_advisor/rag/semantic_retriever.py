# backend/app/ai_advisor/rag/semantic_retriever.py
from datetime import datetime
import json
from typing import List, Dict, Any, Optional
import logging
from .advanced_pdf_processor import ProcessingResult, AdvancedPDFProcessor
from .smart_chunker import SmartChunker
from .pgvector_manager import PgVectorManager
from ..core.hybrid_embedder import HybridEmbedder

logger = logging.getLogger(__name__)

class SemanticRetriever:
    """Ù…Ø³ØªØ±Ø¬Ø¹ Ø¯Ù„Ø§Ù„ÙŠ Ù…ØªÙ‚Ø¯Ù… Ù„Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ© - ÙŠØ¯Ø¹Ù… AWS Textract ÙˆØ§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…Ø­Ù„ÙŠØ©"""
    
    def __init__(self, database_url: str):
        self.vector_db = PgVectorManager(database_url)
        self.embedder = HybridEmbedder()
        self.chunker = SmartChunker()
        self.is_initialized = False
    
    async def initialize(self):
        """ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ù…Ø³ØªØ±Ø¬Ø¹"""
        if not self.is_initialized:
            success = await self.vector_db.initialize()
            if success:
                self.is_initialized = True
                logger.info("âœ… ØªÙ… ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ù…Ø³ØªØ±Ø¬Ø¹ Ø§Ù„Ø¯Ù„Ø§Ù„ÙŠ Ø¨Ù†Ø¬Ø§Ø­")
            else:
                raise Exception("ÙØ´Ù„ ØªÙ‡ÙŠØ¦Ø© Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª")
    
    async def ingest_legal_document(self, pdf_path: str, metadata: Dict[str, Any]) -> Dict[str, Any]:
        """
        Ø§Ø³ØªÙŠØ¹Ø§Ø¨ ÙˆØ«ÙŠÙ‚Ø© Ù‚Ø§Ù†ÙˆÙ†ÙŠØ© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… AWS Textract ÙƒØ®ÙŠØ§Ø± Ø£Ø³Ø§Ø³ÙŠ
        Ù…Ø¹ fallback Ù„Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…Ø­Ù„ÙŠØ© Ø¥Ø°Ø§ ÙØ´Ù„ Ø§Ù„Ø§ØªØµØ§Ù„ Ø¨Ù€ AWS
        """
        try:
            # Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø© Ø§Ù„Ø£ÙˆÙ„Ù‰: Ø§Ø³ØªØ®Ø¯Ø§Ù… AWS Textract Ù„Ù„Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù…ØªÙ‚Ø¯Ù…
            aws_result = await self._ingest_with_aws(pdf_path, metadata)
            if aws_result["success"]:
                logger.info("âœ… ØªÙ… Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…Ø³ØªÙ†Ø¯ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… AWS Textract")
                return aws_result
            
            # Fallback: Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…Ø­Ù„ÙŠØ© Ø¥Ø°Ø§ ÙØ´Ù„ AWS
            logger.info("ğŸ”„ Ø§Ù„Ø§Ù†ØªÙ‚Ø§Ù„ Ù„Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…Ø­Ù„ÙŠØ© (ÙØ´Ù„ AWS)")
            return await self._ingest_locally(pdf_path, metadata)
            
        except Exception as e:
            logger.error(f"âŒ ÙØ´Ù„ Ø§Ø³ØªÙŠØ¹Ø§Ø¨ Ø§Ù„Ù…Ø³ØªÙ†Ø¯: {e}")
            return {"success": False, "error": str(e)}
    
    async def _ingest_with_aws(self, pdf_path: str, metadata: Dict[str, Any]) -> Dict[str, Any]:
        """Ø§Ø³ØªÙŠØ¹Ø§Ø¨ Ø§Ù„Ù…Ø³ØªÙ†Ø¯ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… AWS Textract (Ø§Ù„Ø®ÙŠØ§Ø± Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ)"""
        try:
            from ..aws_services.textract_processor import AWSTextractProcessor
            
            processor = AWSTextractProcessor()
            result = await processor.process_pdf(pdf_path)
            
            return {
                "success": True,
                "processing_engine": "aws_textract",
                "document_id": f"aws_{hash(pdf_path)}",
                "text_extracted": len(result.text),
                "pages_processed": result.metadata.get('total_pages', 0),
                "tables_found": len(result.tables),
                "forms_found": len(result.forms),
                "confidence_score": result.metadata.get('confidence', 0),
                "metadata": {**result.metadata, **metadata},
                "text_preview": result.text[:500] + "..." if len(result.text) > 500 else result.text,
                "features": {
                    "ocr_used": True,
                    "tables_extracted": True,
                    "forms_extracted": True,
                    "cloud_processing": True
                }
            }
            
        except Exception as e:
            logger.warning(f"âš ï¸ ÙØ´Ù„ Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… AWS: {e}")
            return {"success": False, "error": f"AWS Textract failed: {str(e)}"}
    
    async def _ingest_locally(self, pdf_path: str, metadata: Dict[str, Any]) -> Dict[str, Any]:
        """Ø§Ø³ØªÙŠØ¹Ø§Ø¨ Ø§Ù„Ù…Ø³ØªÙ†Ø¯ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…Ø­Ù„ÙŠØ© (fallback)"""
        try:
            processor = AdvancedPDFProcessor()
            # âœ… Ø§Ù„ØªØµØ­ÙŠØ­: Ø§Ø³ØªØ®Ø¯Ø§Ù… process_legal_document Ø¨Ø¯Ù„Ø§Ù‹ Ù…Ù† process_law_pdf
            result = processor.process_legal_document(pdf_path)
            
            # Ø­ÙØ¸ Ø§Ù„Ù…Ø³ØªÙ†Ø¯ ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
            document_id = await self._save_document_to_db(result, metadata)
            
            # ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ Ø£Ø¬Ø²Ø§Ø¡
            chunks_created = await self._chunk_and_save_document_fixed(result, document_id, metadata)
            
            return {
                "success": True,
                "processing_engine": "local_advanced",
                "document_id": document_id,
                "articles_processed": len(result.articles),
                "pages_processed": result.total_pages,
                "chunks_created": chunks_created,
                "stats": result.stats,
                "metadata": {**result.metadata, **metadata},
                "sample_articles": [
                    {
                        "number": article.number,
                        "content_preview": article.content[:200] + "...",
                        "page": article.page,
                        "section": article.section
                    }
                    for article in result.articles[:5]  # Ø£ÙˆÙ„ 5 Ù…ÙˆØ§Ø¯ ÙÙ‚Ø·
                ],
                "features": {
                    "ocr_used": result.metadata.get('ocr_used', False),
                    "tables_extracted": result.stats.get('tables_extracted', 0) > 0,
                    "forms_extracted": False,
                    "cloud_processing": False
                }
            }
            
        except Exception as e:
            logger.error(f"âŒ ÙØ´Ù„ Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…Ø­Ù„ÙŠØ©: {e}")
            return {"success": False, "error": f"Local processing failed: {str(e)}"}
    
    async def _save_document_to_db(self, result: ProcessingResult, metadata: Dict[str, Any]) -> int:
        """Ø­ÙØ¸ Ø§Ù„Ù…Ø³ØªÙ†Ø¯ ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª"""
        try:
            async with self.vector_db.pool.acquire() as conn:
                document_id = await conn.fetchval('''
                    INSERT INTO ai_legal_documents (
                        title, content, metadata, document_type, 
                        country, file_path, file_size, processing_stats,
                        created_at, updated_at
                    ) VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9, $10)
                    RETURNING id
                ''', 
                    metadata.get('title', 'Untitled'),
                    result.full_text,
                    json.dumps({**result.metadata, **metadata}),
                    metadata.get('document_type', 'law'),
                    metadata.get('country', 'unknown'),
                    metadata.get('file_path'),
                    metadata.get('file_size', 0),
                    json.dumps(result.stats),
                    datetime.now(),
                    datetime.now()
                )
                
                logger.info(f"ğŸ’¾ ØªÙ… Ø­ÙØ¸ Ø§Ù„Ù…Ø³ØªÙ†Ø¯ ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª (ID: {document_id})")
                return document_id
                
        except Exception as e:
            logger.error(f"âŒ ÙØ´Ù„ Ø­ÙØ¸ Ø§Ù„Ù…Ø³ØªÙ†Ø¯ ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª: {e}")
            raise Exception(f"ÙØ´Ù„ Ø­ÙØ¸ Ø§Ù„Ù…Ø³ØªÙ†Ø¯: {e}")
    
    async def _chunk_and_save_document_fixed(self, result: ProcessingResult, document_id: int, metadata: Dict[str, Any]) -> int:
        """ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ù…Ø³ØªÙ†Ø¯ Ø¥Ù„Ù‰ Ø£Ø¬Ø²Ø§Ø¡ ÙˆØ­ÙØ¸Ù‡Ø§ - Ø§Ù„Ø¥ØµØ¯Ø§Ø± Ø§Ù„Ù…ØµØ­Ø­ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ"""
        try:
            chunks_created = 0
            
            # 1. ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ù†Øµ Ø§Ù„ÙƒØ§Ù…Ù„ Ø¥Ù„Ù‰ Ø£Ø¬Ø²Ø§Ø¡
            full_text_chunks = self.chunker.chunk_text(result.full_text)
            
            for i, chunk_text in enumerate(full_text_chunks):
                if not chunk_text.strip():
                    continue
                    
                # Ø¥Ù†Ø´Ø§Ø¡ ØªØ¶Ù…ÙŠÙ† Ù„Ù„Ø¬Ø²Ø¡
                chunk_embedding = await self.embedder.get_embedding(chunk_text)
                
                # âœ… Ø§Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ø¢Ù…Ù† Ù„Ù„ØªØ¶Ù…ÙŠÙ†
                if hasattr(chunk_embedding, 'tolist'):
                    embedding_list = chunk_embedding.tolist()
                else:
                    embedding_list = list(chunk_embedding)
                
                # âœ… Ø§Ù„ØªØ£ÙƒØ¯ Ù…Ù† Ø£Ù† Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø¹Ù†Ø§ØµØ± Ù‡ÙŠ floats
                embedding_list = [float(x) for x in embedding_list]
                
                # âœ… Ø§Ù„ØªØ­ÙˆÙŠÙ„ Ø¥Ù„Ù‰ string format
                embedding_str = '[' + ','.join(map(str, embedding_list)) + ']'
                
                # Ø­ÙØ¸ Ø§Ù„Ø¬Ø²Ø¡ ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
                async with self.vector_db.pool.acquire() as conn:
                    await conn.execute('''
                        INSERT INTO ai_document_chunks 
                        (document_id, chunk_text, embedding, metadata, created_at)
                        VALUES ($1, $2, $3::vector, $4, $5)
                    ''', 
                        document_id,
                        chunk_text,
                        embedding_str,  # âœ… Ø¥Ø±Ø³Ø§Ù„ Ùƒ string Ù…Ø¹ ØªØ­ÙˆÙŠÙ„ Ø¥Ù„Ù‰ vector
                        json.dumps({
                            **metadata,
                            "chunk_index": i,
                            "total_chunks": len(full_text_chunks),
                            "chunk_type": "full_text",
                            "processing_engine": result.stats.get('processing_engine', 'unknown')
                        }),
                        datetime.now()
                    )
                chunks_created += 1
            
            # 2. Ø­ÙØ¸ Ø§Ù„Ù…ÙˆØ§Ø¯ ÙƒØ£Ø¬Ø²Ø§Ø¡ Ù…Ù†ÙØµÙ„Ø©
            if result.articles:
                for article in result.articles:
                    if article.content and len(article.content.strip()) > 10:
                        article_embedding = await self.embedder.get_embedding(article.content)
                        
                        # âœ… Ø§Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ø¢Ù…Ù† Ù„Ù„ØªØ¶Ù…ÙŠÙ†
                        if hasattr(article_embedding, 'tolist'):
                            article_embedding_list = article_embedding.tolist()
                        else:
                            article_embedding_list = list(article_embedding)
                        
                        # âœ… Ø§Ù„ØªØ£ÙƒØ¯ Ù…Ù† Ø£Ù† Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø¹Ù†Ø§ØµØ± Ù‡ÙŠ floats
                        article_embedding_list = [float(x) for x in article_embedding_list]
                        
                        # âœ… Ø§Ù„ØªØ­ÙˆÙŠÙ„ Ø¥Ù„Ù‰ string format
                        article_embedding_str = '[' + ','.join(map(str, article_embedding_list)) + ']'
                        
                        async with self.vector_db.pool.acquire() as conn:
                            await conn.execute('''
                                INSERT INTO ai_document_chunks 
                                (document_id, chunk_text, embedding, metadata, article_number, created_at)
                                VALUES ($1, $2, $3::vector, $4, $5, $6)
                            ''', 
                                document_id,
                                article.content,
                                article_embedding_str,  # âœ… Ø¥Ø±Ø³Ø§Ù„ Ùƒ string Ù…Ø¹ ØªØ­ÙˆÙŠÙ„ Ø¥Ù„Ù‰ vector
                                json.dumps({
                                    **metadata,
                                    "article_number": article.number,
                                    "article_page": article.page,
                                    "article_section": article.section,
                                    "chunk_type": "article",
                                    "processing_engine": result.stats.get('processing_engine', 'unknown')
                                }),
                                article.number,
                                datetime.now()
                            )
                        chunks_created += 1
            
            logger.info(f"âœ‚ï¸ ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ {chunks_created} Ø¬Ø²Ø¡ Ù…Ù† Ø§Ù„Ù…Ø³ØªÙ†Ø¯")
            return chunks_created
            
        except Exception as e:
            logger.error(f"âŒ ÙØ´Ù„ ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ù…Ø³ØªÙ†Ø¯: {e}")
            return 0
    
    async def retrieve_relevant_content(self, query: str, max_results: int = 8, 
                                      filters: Optional[Dict[str, Any]] = None) -> List[Dict[str, Any]]:
        """Ø§Ø³ØªØ±Ø¬Ø§Ø¹ Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø°ÙŠ Ø§Ù„ØµÙ„Ø©"""
        try:
            if not self.is_initialized:
                await self.initialize()
            
            # Ø¥Ù†Ø´Ø§Ø¡ ØªØ¶Ù…ÙŠÙ† Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…
            query_embedding = await self.embedder.get_embedding(query)
            
            # ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„ÙÙ„Ø§ØªØ±
            document_type = filters.get('document_type') if filters else None
            country = filters.get('country') if filters else None
            
            # Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ø¯Ù„Ø§Ù„ÙŠ
            relevant_chunks = await self.vector_db.semantic_search(
                query_embedding=query_embedding,
                limit=max_results,
                document_type=document_type,
                country=country,
                similarity_threshold=0.6
            )
            
            # ØªÙ†Ø³ÙŠÙ‚ Ø§Ù„Ù†ØªØ§Ø¦Ø¬
            formatted_results = []
            for chunk in relevant_chunks:
                formatted_results.append({
                    'content': chunk.text,
                    'similarity': chunk.similarity,
                    'metadata': chunk.metadata,
                    'article_number': chunk.metadata.get('article_number'),
                    'document_title': chunk.metadata.get('document_title'),
                    'confidence': self._calculate_confidence(chunk)
                })
            
            logger.info(f"ğŸ” ØªÙ… Ø§Ø³ØªØ±Ø¬Ø§Ø¹ {len(formatted_results)} Ù†ØªÙŠØ¬Ø© Ù„Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…: {query[:50]}...")
            return formatted_results
            
        except Exception as e:
            logger.error(f"âŒ ÙØ´Ù„ Ø§Ø³ØªØ±Ø¬Ø§Ø¹ Ø§Ù„Ù…Ø­ØªÙˆÙ‰: {e}")
            return []
    
    async def hybrid_search(self, query: str, keyword_fallback: bool = True) -> List[Dict[str, Any]]:
        """Ø¨Ø­Ø« Ù‡Ø¬ÙŠÙ† ÙŠØ¬Ù…Ø¹ Ø¨ÙŠÙ† Ø§Ù„Ø¯Ù„Ø§Ù„ÙŠ ÙˆØ§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ©"""
        try:
            # Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ø¯Ù„Ø§Ù„ÙŠ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ
            semantic_results = await self.retrieve_relevant_content(query)
            
            # Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø¶Ø¹ÙŠÙØ©ØŒ Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø¨Ø­Ø« Ø¨Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ©
            if keyword_fallback and (not semantic_results or all(r['similarity'] < 0.7 for r in semantic_results)):
                keyword_results = await self._keyword_search(query)
                return keyword_results
            
            return semantic_results
            
        except Exception as e:
            logger.error(f"âŒ ÙØ´Ù„ Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ù‡Ø¬ÙŠÙ†: {e}")
            return []
    
    async def _keyword_search(self, query: str) -> List[Dict[str, Any]]:
        """Ø¨Ø­Ø« Ø¨Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ© (Ø§Ø­ØªÙŠØ§Ø·ÙŠ)"""
        try:
            async with self.vector_db.pool.acquire() as conn:
                # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ© Ù…Ù† Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…
                keywords = self._extract_keywords(query)
                
                if not keywords:
                    return []
                
                # Ø¨Ù†Ø§Ø¡ Ø§Ø³ØªØ¹Ù„Ø§Ù… Ø§Ù„Ø¨Ø­Ø«
                search_conditions = " OR ".join([f"chunk_text ILIKE '%{kw}%'" for kw in keywords])
                
                rows = await conn.fetch(f'''
                    SELECT 
                        dc.id,
                        dc.chunk_text,
                        dc.metadata,
                        dc.article_number,
                        ld.title as document_title,
                        0.5 as similarity  -- Ø«Ù‚Ø© Ù…ØªÙˆØ³Ø·Ø© Ù„Ù„Ø¨Ø­Ø« Ø¨Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ©
                    FROM ai_document_chunks dc
                    JOIN ai_legal_documents ld ON dc.document_id = ld.id
                    WHERE {search_conditions}
                    LIMIT 10
                ''')
                
                results = []
                for row in rows:
                    results.append({
                        'content': row['chunk_text'],
                        'similarity': row['similarity'],
                        'metadata': json.loads(row['metadata']) if row['metadata'] else {},
                        'article_number': row['article_number'],
                        'document_title': row['document_title'],
                        'confidence': 0.5,
                        'search_type': 'keyword'
                    })
                
                logger.info(f"ğŸ”¤ ØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ {len(results)} Ù†ØªÙŠØ¬Ø© Ø¨Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ©")
                return results
                
        except Exception as e:
            logger.error(f"âŒ ÙØ´Ù„ Ø§Ù„Ø¨Ø­Ø« Ø¨Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ©: {e}")
            return []
    
    def _extract_keywords(self, query: str) -> List[str]:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ© Ù…Ù† Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…"""
        # Ù‚Ø§Ø¦Ù…Ø© Ø¨Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ø´Ø§Ø¦Ø¹Ø© Ø§Ù„ØªÙŠ ÙŠÙ…ÙƒÙ† ØªØ¬Ø§Ù‡Ù„Ù‡Ø§
        stop_words = {'Ù…Ø§', 'Ù‡ÙŠ', 'ÙƒÙŠÙ', 'Ù„Ù…Ø§Ø°Ø§', 'Ù…ØªÙ‰', 'Ø£ÙŠÙ†', 'Ù…Ù†', 'Ù‡Ù„', 'Ø¹Ù„Ù‰', 'ÙÙŠ', 'Ù…Ù†'}
        
        # ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù… Ø¥Ù„Ù‰ ÙƒÙ„Ù…Ø§Øª
        words = query.split()
        
        # ØªØµÙÙŠØ© Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…Ù‡Ù…Ø©
        keywords = [
            word for word in words 
            if len(word) > 2 and word not in stop_words
        ]
        
        return keywords[:5]  # Ø§Ù„Ø­Ø¯ Ø§Ù„Ø£Ù‚ØµÙ‰ 5 ÙƒÙ„Ù…Ø§Øª Ù…ÙØªØ§Ø­ÙŠØ©
    
    def _calculate_confidence(self, chunk: Any) -> float:
        """Ø­Ø³Ø§Ø¨ Ø¯Ø±Ø¬Ø© Ø§Ù„Ø«Ù‚Ø© Ù„Ù„Ù†ØªÙŠØ¬Ø©"""
        base_confidence = chunk.similarity
        
        # Ø²ÙŠØ§Ø¯Ø© Ø§Ù„Ø«Ù‚Ø© Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ø§Ù„Ù…Ø§Ø¯Ø© Ù…Ø­Ø¯Ø¯Ø©
        if chunk.metadata.get('article_number'):
            base_confidence += 0.1
        
        # Ø²ÙŠØ§Ø¯Ø© Ø§Ù„Ø«Ù‚Ø© Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ù†Øµ Ù…Ù† Ù‚Ø§Ù†ÙˆÙ† Ù…Ø¹ØªØ±Ù Ø¨Ù‡
        if chunk.metadata.get('document_type') == 'law':
            base_confidence += 0.1
        
        return min(1.0, base_confidence)
    
    async def get_retrieval_stats(self) -> Dict[str, Any]:
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ø§Ø³ØªØ±Ø¬Ø§Ø¹"""
        if not self.is_initialized:
            await self.initialize()
        
        db_stats = await self.vector_db.get_system_stats()
        embedder_info = self.embedder.get_model_info()
        
        return {
            'database': db_stats,
            'embedding_model': embedder_info,
            'chunker_config': {
                'max_chunk_size': self.chunker.max_chunk_size,
                'overlap': self.chunker.overlap
            },
            'retrieval_ready': self.is_initialized
        }