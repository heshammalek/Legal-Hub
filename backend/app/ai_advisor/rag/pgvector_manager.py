# backend/app/ai_advisor/rag/pgvector_manager.py
import asyncpg
from asyncpg.pool import Pool
import numpy as np
from typing import List, Dict, Any, Optional
import logging
from dataclasses import dataclass
import json
from datetime import datetime
from ..core.hybrid_embedder import HybridEmbedder


logger = logging.getLogger(__name__)

@dataclass
class DocumentChunk:
    id: int
    text: str
    embedding: np.ndarray
    metadata: Dict[str, Any]
    similarity: float = 0.0

class PgVectorManager:
    """ูุฏูุฑ ูุชูุฏู ููุงุนุฏุฉ ุงูุจูุงูุงุช ุงููุชุฌูุฉ ุจุงุณุชุฎุฏุงู PostgreSQL + pgvector"""
    
    def __init__(self, database_url: str):
        self.database_url = database_url
        self.pool = None
    
    async def initialize(self):
        """ุชููุฆุฉ ูุงุนุฏุฉ ุงูุจูุงูุงุช ูุงูุฌุฏุงูู"""
        try:
            # ุชุญููู ูู ุชูุณูู SQLAlchemy ุฅูู ุชูุณูู asyncpg
            if self.database_url.startswith('postgresql+asyncpg://'):
                # ุชุญููู: postgresql+asyncpg://user:pass@host:port/dbname
                # ุฅูู: postgresql://user:pass@host:port/dbname
                asyncpg_url = self.database_url.replace('postgresql+asyncpg://', 'postgresql://')
            else:
                asyncpg_url = self.database_url
            
            logger.info(f"๐ ูุญุงููุฉ ุงูุงุชุตุงู ุจูุงุนุฏุฉ ุงูุจูุงูุงุช: {asyncpg_url}")
            
            self.pool = await asyncpg.create_pool(
                asyncpg_url,
                min_size=5,
                max_size=20
            )
            
            async with self.pool.acquire() as conn:
                # ุงูุชุญูู ูู ูุฌูุฏ pgvector
                await conn.execute("CREATE EXTENSION IF NOT EXISTS vector;")
                await self._create_tables(conn)
                
            logger.info("โ ุชู ุชููุฆุฉ ูุงุนุฏุฉ ุงูุจูุงูุงุช ุงููุชุฌูุฉ ุจูุฌุงุญ")
            return True
            
        except Exception as e:
            logger.error(f"โ ูุดู ุชููุฆุฉ ูุงุนุฏุฉ ุงูุจูุงูุงุช: {e}")
            return False

    async def _create_tables(self, conn):
        """ุฅูุดุงุก ุงูุฌุฏุงูู ุงููุทููุจุฉ"""
        # ุฌุฏูู ุงููุณุชูุฏุงุช ุงููุงููููุฉ (ูุญุฏุซ)
        await conn.execute('''
            CREATE TABLE IF NOT EXISTS ai_legal_documents (
                id SERIAL PRIMARY KEY,
                title VARCHAR(500) NOT NULL,
                content TEXT,
                metadata JSONB,
                document_type VARCHAR(100),
                country VARCHAR(100),
                file_path VARCHAR(1000),
                file_size INTEGER,
                processing_stats JSONB,
                created_at TIMESTAMP DEFAULT NOW(),
                updated_at TIMESTAMP DEFAULT NOW()
            )
        ''')
        
        # ุฌุฏูู ุฃุฌุฒุงุก ุงููุณุชูุฏุงุช ูุน ุงูุชุถูููุงุช (ูุญุฏุซ)
        await conn.execute('''
            CREATE TABLE IF NOT EXISTS ai_document_chunks (
                id SERIAL PRIMARY KEY,
                document_id INTEGER REFERENCES ai_legal_documents(id) ON DELETE CASCADE,
                chunk_text TEXT NOT NULL,
                embedding VECTOR(768),
                metadata JSONB,
                article_number VARCHAR(50),
                created_at TIMESTAMP DEFAULT NOW()
            )
        ''')
        
        # ุฅูุดุงุก ุงูููุงุฑุณ
        await conn.execute('''
            CREATE INDEX IF NOT EXISTS idx_document_chunks_embedding 
            ON ai_document_chunks USING ivfflat (embedding vector_cosine_ops)
        ''')
        
        await conn.execute('''
            CREATE INDEX IF NOT EXISTS idx_document_type ON ai_legal_documents(document_type)
        ''')
        
        await conn.execute('''
            CREATE INDEX IF NOT EXISTS idx_country ON ai_legal_documents(country)
        ''')
        
        await conn.execute('''
            CREATE INDEX IF NOT EXISTS idx_article_number ON ai_document_chunks(article_number)
        ''')
        
        logger.info("โ ุชู ุฅูุดุงุก/ุงูุชุฃูุฏ ูู ุงูุฌุฏุงูู ูุงูููุงุฑุณ")

    async def store_document(self, metadata: Dict[str, Any]) -> int:
        """ุชุฎุฒูู ูุณุชูุฏ ุฌุฏูุฏ"""
        async with self.pool.acquire() as conn:
            document_id = await conn.fetchval('''
                INSERT INTO ai_legal_documents 
                (title, document_type, source_url, jurisdiction, language, file_path, file_size, page_count, processing_status)
                VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9)
                RETURNING id
            ''', 
                metadata.get('title', 'Unknown'),
                metadata.get('document_type', 'law'),
                metadata.get('source_url'),
                metadata.get('jurisdiction', 'EG'),
                metadata.get('language', 'ar'),
                metadata.get('file_path'),
                metadata.get('file_size'),
                metadata.get('page_count'),
                'processed'
            )
            
            return document_id

    async def store_chunks_with_embeddings_fixed(self, document_id: int, chunks: List[Any], embedder: HybridEmbedder):
        """ุชุฎุฒูู ุงูุฃุฌุฒุงุก ูุน ุงูุชุถูููุงุช - ุงูุฅุตุฏุงุฑ ุงููุตุญุญ ุงูููุงุฆู"""
        try:
            # ุงุณุชุฎุฑุงุฌ ุงููุตูุต ูู ุงูู chunks
            texts = [chunk.text for chunk in chunks]
            
            # ุฅูุดุงุก ุงูุชุถูููุงุช
            embeddings = await embedder.get_embeddings(texts)
            
            async with self.pool.acquire() as conn:
                async with conn.transaction():
                    for i, (chunk, embedding) in enumerate(zip(chunks, embeddings)):
                        # โ ุงูุชุญููู ุงูุขูู ููุชุถููู
                        if hasattr(embedding, 'tolist'):
                            embedding_list = embedding.tolist()
                        else:
                            embedding_list = list(embedding)
                        
                        # โ ุงูุชุฃูุฏ ูู ุฃู ุฌููุน ุงูุนูุงุตุฑ ูู floats
                        embedding_list = [float(x) for x in embedding_list]
                        
                        # โ ุงูุชุญููู ุฅูู string format ุงูุฐู ูุชุนุฑู ุนููู pgvector
                        # ุงูุชูุณูู ุงูุตุญูุญ: '[0.1, 0.2, 0.3]'
                        embedding_str = '[' + ','.join(map(str, embedding_list)) + ']'
                        
                        await conn.execute('''
                            INSERT INTO ai_document_chunks 
                            (document_id, chunk_text, embedding, metadata, article_number, created_at)
                            VALUES ($1, $2, $3::vector, $4, $5, $6)
                        ''', 
                            document_id,
                            chunk.text,
                            embedding_str,  # โ ุฅุฑุณุงู ู string ูุน ุชุญููู ุฅูู vector
                            json.dumps(chunk.metadata),
                            chunk.metadata.get('article_number'),
                            datetime.now()
                        )
            
            logger.info(f"โ ุชู ุชุฎุฒูู {len(chunks)} ุฌุฒุก ูููุณุชูุฏ {document_id}")
            
        except Exception as e:
            logger.error(f"โ ูุดู ุชุฎุฒูู ุงูุฃุฌุฒุงุก: {e}")
            raise

    async def semantic_search(self, query_embedding: np.ndarray, limit: int = 10, 
    document_type: str = None, similarity_threshold: float = 0.7) -> List[DocumentChunk]:
        
        """ุจุญุซ ุฏูุงูู ูุชูุฏู"""
        try:
            # ุชุญููู numpy array ุฅูู list ุจุดูู ุตุญูุญ
            if hasattr(query_embedding, 'tolist'):
                embedding_list = query_embedding.tolist()
            else:
                embedding_list = list(query_embedding)
            
            # ุชุฃูุฏ ุฃู ุงูุชุถููู ูู list of floats
            if not all(isinstance(x, (int, float)) for x in embedding_list):
                logger.error("โ ุชูุณูู ุงูุชุถููู ุบูุฑ ุตุญูุญ")
                return []

            # ุชู ุงูุชุญูู ูู ุงูุชูุณููุ ููุงู ูุฅุถุงูุฉ ุชูููุฐ ุงูุจุญุซ ุงูุฏูุงูู ูุงุญููุง
            logger.debug("โ ุงุณุชููุช ุชุถููู ุงูุจุญุซ ุจูุฌุงุญ")
            return []
        except Exception as e:
            logger.error(f"โ ูุดู ุนูููุฉ ุงูุจุญุซ ุงูุฏูุงูู: {e}")
            return []

   
    
    async def get_document_stats(self, document_id: int) -> Dict[str, Any]:
        """ุงูุญุตูู ุนูู ุฅุญุตุงุฆูุงุช ุงููุณุชูุฏ"""
        async with self.pool.acquire() as conn:
            stats = await conn.fetchrow('''
                SELECT 
                    COUNT(*) as total_chunks,
                    AVG(token_count) as avg_tokens,
                    COUNT(DISTINCT article_number) as unique_articles,
                    MIN(created_at) as first_chunk,
                    MAX(created_at) as last_chunk
                FROM ai_document_chunks 
                WHERE document_id = $1
            ''', document_id)
            
            return dict(stats) if stats else {}
    
    async def get_system_stats(self) -> Dict[str, Any]:
        """ุงูุญุตูู ุนูู ุฅุญุตุงุฆูุงุช ุงููุธุงู"""
        async with self.pool.acquire() as conn:
            # ุฅุญุตุงุฆูุงุช ุงููุณุชูุฏุงุช
            doc_stats = await conn.fetchrow('''
                SELECT 
                    COUNT(*) as total_documents,
                    COUNT(DISTINCT jurisdiction) as unique_jurisdictions,
                    SUM(page_count) as total_pages
                FROM ai_legal_documents
                WHERE processing_status = 'processed'
            ''')
            
            # ุฅุญุตุงุฆูุงุช ุงูุฃุฌุฒุงุก
            chunk_stats = await conn.fetchrow('''
                SELECT 
                    COUNT(*) as total_chunks,
                    SUM(token_count) as total_tokens,
                    AVG(token_count) as avg_tokens_per_chunk
                FROM ai_document_chunks
            ''')
            
            return {
                'documents': dict(doc_stats) if doc_stats else {},
                'chunks': dict(chunk_stats) if chunk_stats else {},
                'database_size': await self._get_database_size(conn)
            }
    
    async def _get_database_size(self, conn) -> str:
        """ุงูุญุตูู ุนูู ุญุฌู ูุงุนุฏุฉ ุงูุจูุงูุงุช"""
        size = await conn.fetchval("SELECT pg_size_pretty(pg_database_size(current_database()));")
        return size
    
    async def cleanup_old_data(self, days_old: int = 30):
        """ุชูุธูู ุงูุจูุงูุงุช ุงููุฏููุฉ (ุจูุงุกู ุนูู ุชุงุฑูุฎ ุงูุฅูุดุงุก)"""
        try:
            async with self.pool.acquire() as conn:
                # ุงูุงุณุชุนูุงู ุงููุนุฏู ูุงุณุชุฎุฏุงู $1 ููุนุงูู ุตุญูุญ
                # ูุงุณุชุฎุฏุงู RETURNING COUNT(*) ููุญุตูู ุนูู ุงูุนุฏุฏ ูุจุงุดุฑุฉ
                deleted_count = await conn.fetchval(
                    '''
                    DELETE FROM ai_legal_documents 
                    WHERE created_at < (NOW() - ($1 * INTERVAL '1 day'))
                    RETURNING COUNT(*)
                    ''', 
                    days_old
                )
                
                count = deleted_count if deleted_count else 0
                logger.info(f"๐งน ุชู ุชูุธูู {count} ูุณุชูุฏ ูุฏูู (ุฃูุฏู ูู {days_old} ููู)")
                return count
        
        except Exception as e:
            logger.error(f"โ ูุดู ุนูููุฉ ุชูุธูู ุงูุจูุงูุงุช ุงููุฏููุฉ: {e}")
            return 
        # ูู ุฏุงูุฉ cleanup_old_data - ุฅุตูุงุญ ุงูุงุณุชุนูุงู:
async def cleanup_old_data(self, days_old: int = 30):
    """ุชูุธูู ุงูุจูุงูุงุช ุงููุฏููุฉ"""
    try:
        async with self.pool.acquire() as conn:
            # ุฅุตูุงุญ ุงูุงุณุชุนูุงู - ุงุณุชุฎุฏุงู ูุนุงูู ุตุญูุญ ุจุดูู ุตุญูุญ
            deleted_count = await conn.fetchval(
                '''
                WITH deleted AS (
                    DELETE FROM ai_legal_documents 
                    WHERE created_at < (NOW() - ($1 * INTERVAL '1 day'))
                    RETURNING id
                )
                SELECT COUNT(*) FROM deleted
                ''', 
                days_old
            )
            
            logger.info(f"๐งน ุชู ุชูุธูู {deleted_count} ูุณุชูุฏ ูุฏูู (ุฃูุฏู ูู {days_old} ููู)")
            return deleted_count
            
    except Exception as e:
        logger.error(f"โ ูุดู ุนูููุฉ ุชูุธูู ุงูุจูุงูุงุช ุงููุฏููุฉ: {e}")
        return 0


        