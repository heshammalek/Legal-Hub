# backend/app/ai_advisor/rag/advanced_pdf_processor.py
import os
import logging
from pathlib import Path
import requests
import json
from dataclasses import dataclass
from typing import List, Dict, Any, Optional
import re

logger = logging.getLogger(__name__)

@dataclass
class LegalArticle:
    number: str
    content: str
    page: int
    full_text: str = ""
    section: str = ""
    tokens: int = 0

@dataclass
class ProcessingResult:
    articles: List[LegalArticle]
    sections: List[str]
    full_text: str
    total_pages: int
    stats: Dict[str, Any]
    metadata: Dict[str, Any]

class AdvancedPDFProcessor:
    """Ù…Ø¹Ø§Ù„Ø¬ PDF Ù…ØªØ¹Ø¯Ø¯ Ø§Ù„Ø®ÙŠØ§Ø±Ø§Øª - Unstructured OSS / Docling / Marker / pymupdf / OCR"""
    
    def __init__(self):
        self.processors = self._detect_available_processors()
        logger.info(f"ğŸ› ï¸ ØªØ±ØªÙŠØ¨ Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø§Øª: {[p[0] for p in self.processors]}")
        
    def _detect_available_processors(self) -> List:
        """ÙƒØ´Ù Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø§Øª Ø§Ù„Ù…ØªØ§Ø­Ø© ÙˆØªØ±ØªÙŠØ¨Ù‡Ø§ Ø­Ø³Ø¨ Ø§Ù„Ø£ÙˆÙ„ÙˆÙŠØ©"""
        processors = []
        
        # 1. Unstructured Open Source (Ø§Ù„Ù…Ø¬Ø§Ù†ÙŠ ÙˆØ§Ù„Ù…Ø³ØªÙ‚Ø±)
        if self._check_unstructured_oss():
            processors.append(("unstructured_oss", self._process_with_unstructured_oss))
        
        # 2. Docling (Ø§Ù„Ø£Ø­Ø¯Ø«)
        if self._check_docling():
            processors.append(("docling", self._process_with_docling))
        
        # 3. Marker (Ù…Ø­Ù„ÙŠ Ø¬ÙŠØ¯)
        if self._check_marker_local():
            processors.append(("marker", self._process_with_marker_local))
        
        # 4. pymupdf (Ø¨Ø¯ÙŠÙ„ Ø¢Ù…Ù† Ø¯Ø§Ø¦Ù…Ø§Ù‹)
        if self._check_pymupdf():
            processors.append(("pymupdf", self._process_with_pymupdf))
            
        # 5. OCR ÙƒØ­Ù„ Ø£Ø®ÙŠØ± Ù„Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª Ø§Ù„Ù…Ù…Ø³ÙˆØ­Ø©
        if self._check_ocr_capability():
            processors.append(("ocr_fallback", self._process_with_ocr))
            
        return processors

    def _check_ocr_capability(self) -> bool:
        """Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø¥Ù…ÙƒØ§Ù†ÙŠØ© OCR"""
        try:
            import pytesseract
            from PIL import Image
            import pdf2image
            return True
        except ImportError:
            return False

    def _process_with_ocr(self, pdf_path: str) -> ProcessingResult:
        """Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… OCR Ù„Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª Ø§Ù„Ù…Ù…Ø³ÙˆØ­Ø©"""
        try:
            import pytesseract
            from PIL import Image
            import pdf2image
            
            logger.info("ğŸ” Ø§Ø³ØªØ®Ø¯Ø§Ù… OCR Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ù…Ø³ØªÙ†Ø¯ Ù…Ù…Ø³ÙˆØ­...")
            
            full_text = ""
            images = pdf2image.convert_from_path(pdf_path, dpi=300)
            
            for i, image in enumerate(images):
                page_text = pytesseract.image_to_string(image, lang='ara+eng')
                full_text += f"\n--- Ø§Ù„ØµÙØ­Ø© {i+1} ---\n{page_text}"
            
            if not full_text.strip():
                raise Exception("Ù„Ù… ÙŠØªÙ… Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø£ÙŠ Ù†Øµ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… OCR")
            
            articles = self._extract_articles_enhanced(full_text)
            
            return ProcessingResult(
                articles=articles,
                sections=self._extract_sections(full_text),
                full_text=full_text,
                total_pages=len(images),
                stats={
                    "total_articles": len(articles),
                    "processing_engine": "ocr_tesseract",
                    "pages_processed": len(images)
                },
                metadata={
                    "file_size": os.path.getsize(pdf_path),
                    "ocr_used": True,
                    "dpi": 300
                }
            )
            
        except Exception as e:
            raise Exception(f"ÙØ´Ù„ OCR: {e}")

    def process_legal_document(self, file_path: str) -> ProcessingResult:
        """Ù…Ø¹Ø§Ù„Ø¬Ø© Ø£ÙŠ Ù†ÙˆØ¹ Ù…Ù† Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ©"""
        file_ext = Path(file_path).suffix.lower()
        
        # Ø¥Ø°Ø§ ÙƒØ§Ù† Ù…Ù„Ù Ù†ØµÙŠØŒ Ù…Ø¹Ø§Ù„Ø¬ØªÙ‡ Ù…Ø¨Ø§Ø´Ø±Ø©
        if file_ext in {'.txt', '.md'}:
            return self.process_text_file(file_path)
        
        # Ø¥Ø°Ø§ ÙƒØ§Ù† PDF Ø£Ùˆ ØµÙˆØ±ØŒ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø§Øª Ø§Ù„Ù…ØªØ¹Ø¯Ø¯Ø©
        elif file_ext in {'.pdf', '.jpg', '.jpeg', '.png', '.tiff', '.bmp'}:
            for processor_name, processor_func in self.processors:
                try:
                    logger.info(f"ğŸ”„ ØªØ¬Ø±Ø¨Ø© Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù…: {processor_name}")
                    result = processor_func(file_path)
                    
                    # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø£Ù† Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ù„ÙŠØ³Øª ÙØ§Ø±ØºØ©
                    if not result.articles and result.full_text.strip():
                        logger.warning(f"âš ï¸ Ù„Ù… ØªØ³ØªØ®Ø±Ø¬ {processor_name} Ø£ÙŠ Ù…ÙˆØ§Ø¯ØŒ Ø¬Ø§Ø±ÙŠ Ø§Ø³ØªØ®Ø±Ø§Ø¬ ÙŠØ¯ÙˆÙŠ...")
                        result.articles = self._extract_articles_enhanced(result.full_text)
                    
                    if result.articles or len(result.full_text.strip()) > 100:
                        logger.info(f"âœ… Ù†Ø¬Ø­ Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù…: {processor_name} - ØªÙ… Ø§Ø³ØªØ®Ø±Ø§Ø¬ {len(result.articles)} Ù…Ø§Ø¯Ø©")
                        return result
                    else:
                        logger.warning(f"âš ï¸ {processor_name} Ù„Ù… ÙŠØ³ØªØ®Ø±Ø¬ Ù…Ø­ØªÙˆÙ‰ ÙƒØ§ÙÙŠ")
                        continue
                        
                except Exception as e:
                    logger.warning(f"âš ï¸ ÙØ´Ù„ {processor_name}: {e}")
                    continue
                    
            raise Exception("âŒ ÙØ´Ù„ Ø¬Ù…ÙŠØ¹ Ù…Ø¹Ø§Ù„Ø¬Ø§Øª Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª Ø§Ù„Ù…ØªØ§Ø­Ø©")
        
        else:
            raise Exception(f"âŒ Ù†ÙˆØ¹ Ø§Ù„Ù…Ù„Ù ØºÙŠØ± Ù…Ø¯Ø¹ÙˆÙ…: {file_ext}")

    def process_text_file(self, file_path: str) -> ProcessingResult:
        """Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù†ØµÙŠØ© (.txt, .md)"""
        try:
            logger.info(f"ğŸ“„ Ù…Ø¹Ø§Ù„Ø¬Ø© Ù…Ù„Ù Ù†ØµÙŠ: {file_path}")
            
            # Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„Ù…Ù„Ù Ø§Ù„Ù†ØµÙŠ
            with open(file_path, 'r', encoding='utf-8') as file:
                full_text = file.read()
            
            if not full_text.strip():
                raise Exception("Ø§Ù„Ù…Ù„Ù Ø§Ù„Ù†ØµÙŠ ÙØ§Ø±Øº Ø£Ùˆ Ù„Ø§ ÙŠÙ…ÙƒÙ† Ù‚Ø±Ø§Ø¡ØªÙ‡")
            
            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù…ÙˆØ§Ø¯
            articles = self._extract_articles_enhanced(full_text)
            
            return ProcessingResult(
                articles=articles,
                sections=self._extract_sections(full_text),
                full_text=full_text,
                total_pages=1,
                stats={
                    "total_articles": len(articles),
                    "processing_engine": "text_file",
                    "file_type": Path(file_path).suffix
                },
                metadata={
                    "file_size": os.path.getsize(file_path),
                    "encoding": "utf-8",
                    "is_text_file": True
                }
            )
            
        except Exception as e:
            raise Exception(f"ÙØ´Ù„ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…Ù„Ù Ø§Ù„Ù†ØµÙŠ: {e}")

    # âœ… Ø¥Ø¶Ø§ÙØ© Ø·Ø±ÙŠÙ‚Ø© Ø§Ù„ØªÙˆØ§ÙÙ‚ Ù…Ø¹ Ø§Ù„ÙƒÙˆØ¯ Ø§Ù„Ù‚Ø¯ÙŠÙ…
    def process_law_pdf(self, file_path: str) -> ProcessingResult:
        """Ø·Ø±ÙŠÙ‚Ø© ØªÙˆØ§ÙÙ‚ÙŠØ© - ØªØ³ØªØ¯Ø¹ÙŠ process_legal_document"""
        return self.process_legal_document(file_path)

    # ================== Ø§Ù„Ø®ÙŠØ§Ø± 1: Unstructured Open Source (Ø§Ù„Ù…Ø¬Ø§Ù†ÙŠ) ==================
    def _process_with_unstructured_oss(self, pdf_path: str) -> ProcessingResult:
        """
        Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Unstructured Open Source - Ø§Ù„Ù…Ø³ØªÙ‚Ø± ÙˆØ§Ù„Ù…Ø¬Ø§Ù†ÙŠ
        Ù„Ù„ØªØ«Ø¨ÙŠØª: pip install "unstructured[pdf]"
        """
        try:
            from unstructured.partition.pdf import partition_pdf
            
            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø¹Ù†Ø§ØµØ± Ù…Ù† PDF
            elements = partition_pdf(
                filename=pdf_path,
                extract_images=False,
                strategy="auto",  # auto, fast, hi_res, ocr_only
                languages=["ara", "eng"],
                include_page_breaks=True
            )
            
            # ØªØ¬Ù…ÙŠØ¹ Ø§Ù„Ù†Øµ ÙˆØ§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
            full_text = ""
            tables_data = []
            sections = []
            
            for element in elements:
                element_text = getattr(element, 'text', '')
                if element_text:
                    full_text += element_text + "\n\n"
                
                # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø£Ù‚Ø³Ø§Ù…
                if hasattr(element, 'category') and element.category == "Title":
                    sections.append(element_text)
                
                # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø¬Ø¯Ø§ÙˆÙ„
                if hasattr(element, 'category') and element.category == "Table":
                    tables_data.append({
                        'text': element_text,
                        'metadata': element.metadata.to_dict() if hasattr(element, 'metadata') else {}
                    })
            
            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ØµÙØ­Ø§Øª
            page_numbers = set()
            for element in elements:
                if hasattr(element, 'metadata') and hasattr(element.metadata, 'page_number'):
                    page_numbers.add(element.metadata.page_number)
            
            total_pages = max(page_numbers) if page_numbers else 1
            
            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù…ÙˆØ§Ø¯
            articles = self._extract_articles_enhanced(full_text)
            
            return ProcessingResult(
                articles=articles,
                sections=sections[:10],  # Ø£ÙˆÙ„ 10 Ø£Ù‚Ø³Ø§Ù… ÙÙ‚Ø·
                full_text=full_text,
                total_pages=total_pages,
                stats={
                    "total_articles": len(articles),
                    "tables_extracted": len(tables_data),
                    "elements_found": len(elements),
                    "processing_engine": "unstructured_oss",
                    "strategy": "auto"
                },
                metadata={
                    "tables": tables_data,
                    "elements_categories": [e.category for e in elements if hasattr(e, 'category')],
                    "file_size": os.path.getsize(pdf_path),
                    "total_pages": total_pages
                }
            )
            
        except ImportError:
            raise Exception('unstructured ØºÙŠØ± Ù…Ø«Ø¨Øª. run: pip install "unstructured[pdf]"')
        except Exception as e:
            raise Exception(f"ÙØ´Ù„ unstructured: {e}")

    def _check_unstructured_oss(self) -> bool:
        """Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ØªÙˆÙØ± unstructured-open-source"""
        try:
            from unstructured.partition.pdf  import partition_pdf
            return True
        except ImportError:
            return False

    # ================== Ø§Ù„Ø®ÙŠØ§Ø± 2: Docling ==================
    def _process_with_docling(self, pdf_path: str) -> ProcessingResult:
        """Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Docling"""
        try:
            from docling.document_converter import DocumentConverter
            
            converter = DocumentConverter()
            result = converter.convert(pdf_path)
            markdown_output = result.document.export_to_markdown()
            
            articles = self._extract_articles_enhanced(markdown_output)
            
            return ProcessingResult(
                articles=articles,
                sections=self._extract_sections(markdown_output),
                full_text=markdown_output,
                total_pages=len(result.document.pages) if hasattr(result.document, 'pages') else 1,
                stats={
                    "total_articles": len(articles),
                    "processing_engine": "docling",
                    "markdown_export": True
                },
                metadata={
                    "file_size": os.path.getsize(pdf_path),
                    "converter": "docling"
                }
            )
            
        except ImportError:
            raise Exception("Docling ØºÙŠØ± Ù…Ø«Ø¨Øª. run: pip install docling")
        except Exception as e:
            raise Exception(f"ÙØ´Ù„ Docling: {e}")

    def _check_docling(self) -> bool:
        """Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ØªÙˆÙØ± Docling"""
        try:
            from docling.document_converter import DocumentConverter
            return True
        except ImportError:
            return False

    # ================== Ø§Ù„Ø®ÙŠØ§Ø± 3: Marker ==================
    def _process_with_marker_local(self, pdf_path: str) -> ProcessingResult:
        """Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Marker"""
        try:
            from marker.converters.pdf import PdfConverter
            from marker.models import create_model_dict
            
            converter = PdfConverter(artifact_dict=create_model_dict())
            rendered = converter(pdf_path)
            
            articles = self._extract_articles_enhanced(rendered.markdown)
            
            return ProcessingResult(
                articles=articles,
                sections=self._extract_sections(rendered.markdown),
                full_text=rendered.markdown,
                total_pages=self._count_pages(rendered.metadata),
                stats={
                    "total_articles": len(articles),
                    "processing_engine": "marker_local"
                },
                metadata=rendered.metadata
            )
            
        except ImportError:
            raise Exception("Marker ØºÙŠØ± Ù…Ø«Ø¨Øª. run: pip install marker-pdf[full]")
        except Exception as e:
            raise Exception(f"ÙØ´Ù„ Marker: {e}")

    def _check_marker_local(self) -> bool:
        """Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ØªÙˆÙØ± Marker"""
        try:
            from marker.converters.pdf import PdfConverter
            return True
        except ImportError:
            return False

    # ================== Ø§Ù„Ø®ÙŠØ§Ø± 4: pymupdf ==================
    def _process_with_pymupdf(self, pdf_path: str) -> ProcessingResult:
        """Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… pymupdf"""
        try:
            import fitz
            doc = fitz.open(pdf_path)
            full_text = ""
            for page_num in range(len(doc)):
                page = doc[page_num]
                full_text += page.get_text() + "\n\n"
            
            articles = self._extract_articles_enhanced(full_text)
            
            return ProcessingResult(
                articles=articles,
                sections=self._extract_sections(full_text),
                full_text=full_text,
                total_pages=len(doc),
                stats={
                    "total_articles": len(articles),
                    "processing_engine": "pymupdf"
                },
                metadata={
                    "file_size": os.path.getsize(pdf_path)
                }
            )
        except ImportError:
            raise Exception("pymupdf ØºÙŠØ± Ù…Ø«Ø¨Øª. run: pip install pymupdf")

    def _check_pymupdf(self) -> bool:
        """Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ØªÙˆÙØ± pymupdf"""
        try:
            import fitz
            return True
        except ImportError:
            return False

    # ================== Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…Ø­Ø³Ù‘Ù† Ù„Ù„Ù…ÙˆØ§Ø¯ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ© ==================
    def _extract_articles_enhanced(self, text: str) -> List[LegalArticle]:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…ÙˆØ§Ø¯ Ù‚Ø§Ù†ÙˆÙ†ÙŠØ© Ù…Ø­Ø³Ù‘Ù† Ø¨Ø´Ø¯Ø©"""
        articles = []
        
        # ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù†Øµ
        cleaned_text = self._clean_text(text)
        
        # Ø£Ù†Ù…Ø§Ø· Ù…ØªÙ‚Ø¯Ù…Ø© Ù„Ù„Ù…ÙˆØ§Ø¯ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©
        patterns = [
            # Ø§Ù„Ù†Ù…Ø· 1: "Ø§Ù„Ù…Ø§Ø¯Ø© 1: Ø§Ù„Ù†Øµ..."
            r'Ø§Ù„Ù…Ø§Ø¯Ø©\s+(\d+)[:\-\s]+\s*([^\.]+\.(?:\s+[^\.]+\.)*)(?=\s*Ø§Ù„Ù…Ø§Ø¯Ø©\s+\d+|\s*$|\s*Ù…Ø§Ø¯Ø©\s+\d+)',
            # Ø§Ù„Ù†Ù…Ø· 2: "Ù…Ø§Ø¯Ø© 1 Ø§Ù„Ù†Øµ..."
            r'Ù…Ø§Ø¯Ø©\s+(\d+)[\s]+([^\.]+\.(?:\s+[^\.]+\.)*)(?=\s*Ù…Ø§Ø¯Ø©\s+\d+|\s*$|\s*Ø§Ù„Ù…Ø§Ø¯Ø©\s+\d+)',
            # Ø§Ù„Ù†Ù…Ø· 3: "Article 1: Ø§Ù„Ù†Øµ..."
            r'Article\s+(\d+)[:\-\s]+\s*([^\.]+\.(?:\s+[^\.]+\.)*)(?=\s*Article\s+\d+|\s*$)',
            # Ø§Ù„Ù†Ù…Ø· 4: Ù…Ø¹ Ù†Ù‚Ø·Ø© Ù†Ù‡Ø§ÙŠØ© ÙˆØ§Ø¶Ø­Ø©
            r'Ø§Ù„Ù…Ø§Ø¯Ø©\s+(\d+)[\s\-\:]*([^\.]+\.[^\.]*(?:\.[^\.]*)*)(?=\s*Ø§Ù„Ù…Ø§Ø¯Ø©\s+\d+|\s*$)',
        ]
        
        for pattern in patterns:
            for match in re.finditer(pattern, cleaned_text, re.DOTALL | re.MULTILINE):
                article_number = match.group(1).strip()
                article_content = match.group(2).strip()
                
                # ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù…Ø­ØªÙˆÙ‰
                article_content = self._clean_article_content(article_content)
                
                if len(article_content) > 10:  # ØªØ£ÙƒØ¯ Ø£Ù† Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ù„ÙŠØ³ Ù‚ØµÙŠØ±Ø§Ù‹ Ø¬Ø¯Ø§Ù‹
                    articles.append(LegalArticle(
                        number=article_number,
                        content=article_content,
                        page=1,  # Ø³ÙŠØªÙ… ØªØ­Ø³ÙŠÙ†Ù‡ Ù„Ø§Ø­Ù‚Ø§Ù‹
                        full_text=f"Ø§Ù„Ù…Ø§Ø¯Ø© {article_number}: {article_content}",
                        tokens=len(article_content.split())
                    ))
        
        # Ø¥Ø°Ø§ Ù„Ù… Ù†Ø¬Ø¯ Ù…ÙˆØ§Ø¯ØŒ Ù†Ø­Ø§ÙˆÙ„ Ø§Ø³ØªØ®Ø±Ø§Ø¬ ÙÙ‚Ø±Ø§Øª Ø·ÙˆÙŠÙ„Ø©
        if not articles:
            articles = self._extract_fallback_articles(cleaned_text)
        
        # ØªØ³Ø¬ÙŠÙ„ Ø§Ù„Ù†ØªØ§Ø¦Ø¬
        if articles:
            logger.info(f"ğŸ“„ ØªÙ… Ø§Ø³ØªØ®Ø±Ø§Ø¬ {len(articles)} Ù…Ø§Ø¯Ø© Ù‚Ø§Ù†ÙˆÙ†ÙŠØ©")
            for i, article in enumerate(articles[:3]):  # Ø£ÙˆÙ„ 3 Ù…ÙˆØ§Ø¯ ÙÙ‚Ø· Ù„Ù„ØªØ³Ø¬ÙŠÙ„
                logger.debug(f"  Ø§Ù„Ù…Ø§Ø¯Ø© {article.number}: {article.content[:100]}...")
        else:
            logger.warning("âš ï¸ Ù„Ù… ÙŠØªÙ… Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø£ÙŠ Ù…ÙˆØ§Ø¯ Ù‚Ø§Ù†ÙˆÙ†ÙŠØ© Ù…Ù† Ø§Ù„Ù†Øµ")
            logger.debug(f"ğŸ“ Ø¹ÙŠÙ†Ø© Ù…Ù† Ø§Ù„Ù†Øµ: {cleaned_text[:500]}...")
        
        return articles

    def _extract_fallback_articles(self, text: str) -> List[LegalArticle]:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø¨Ø¯ÙŠÙ„ Ø¥Ø°Ø§ Ù„Ù… ØªÙˆØ¬Ø¯ Ù…ÙˆØ§Ø¯ ÙˆØ§Ø¶Ø­Ø©"""
        articles = []
        
        # ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙÙ‚Ø±Ø§Øª Ø·ÙˆÙŠÙ„Ø©
        paragraphs = re.split(r'\n\s*\n', text)
        
        for i, paragraph in enumerate(paragraphs):
            paragraph = paragraph.strip()
            if len(paragraph) > 50:  # ÙÙ‚Ø±Ø§Øª Ø·ÙˆÙŠÙ„Ø© ÙÙ‚Ø·
                articles.append(LegalArticle(
                    number=str(i + 1),
                    content=paragraph,
                    page=1,
                    full_text=paragraph,
                    tokens=len(paragraph.split())
                ))
        
        return articles[:20]  # Ø­Ø¯ Ø£Ù‚ØµÙ‰ 20 ÙÙ‚Ø±Ø©

    def _clean_text(self, text: str) -> str:
        """ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù†Øµ"""
        # Ø¥Ø²Ø§Ù„Ø© Ù…Ø³Ø§ÙØ§Øª Ø²Ø§Ø¦Ø¯Ø©
        text = re.sub(r'\s+', ' ', text)
        # Ø§Ù„ØªØ£ÙƒØ¯ Ù…Ù† ÙˆØ¬ÙˆØ¯ Ù…Ø³Ø§ÙØ§Øª Ø¨Ø¹Ø¯ Ø§Ù„Ù†Ù‚Ø§Ø·
        text = re.sub(r'\.([^\s])', r'. \1', text)
        return text.strip()

    def _clean_article_content(self, content: str) -> str:
        """ØªÙ†Ø¸ÙŠÙ Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ù…Ø§Ø¯Ø©"""
        # Ø¥Ø²Ø§Ù„Ø© Ø±Ù…ÙˆØ² Ø®Ø§ØµØ©
        content = re.sub(r'[â€¢\-\*ï®]', '', content)
        # Ø¥Ø²Ø§Ù„Ø© Ù…Ø³Ø§ÙØ§Øª Ø²Ø§Ø¦Ø¯Ø©
        content = re.sub(r'\s+', ' ', content)
        # Ù‚Øµ Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø¥Ø°Ø§ ÙƒØ§Ù† Ø·ÙˆÙŠÙ„Ø§Ù‹ Ø¬Ø¯Ø§Ù‹
        if len(content) > 2000:
            content = content[:2000] + "..."
        return content.strip()

    def _extract_sections(self, text: str) -> List[str]:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø£Ù‚Ø³Ø§Ù…"""
        sections = []
        # Ø£Ù†Ù…Ø§Ø· Ø§Ù„Ø¹Ù†Ø§ÙˆÙŠÙ†
        patterns = [
            r'# (.+?)$',
            r'## (.+?)$',
            r'^(?:Ø§Ù„ÙØµÙ„|Ø§Ù„Ø¨Ø§Ø¨|Ø§Ù„Ù‚Ø³Ù…)\s+(.+?)$'
        ]
        
        for pattern in patterns:
            for match in re.finditer(pattern, text, re.MULTILINE):
                section = match.group(1).strip()
                if section and len(section) > 3:
                    sections.append(section)
        
        return sections[:20]  # Ø£ÙˆÙ„ 20 Ù‚Ø³Ù… ÙÙ‚Ø·

    def _count_pages(self, metadata: Dict) -> int:
        """Ø¹Ø¯ Ø§Ù„ØµÙØ­Ø§Øª"""
        return len(metadata.get('page_stats', [1]))