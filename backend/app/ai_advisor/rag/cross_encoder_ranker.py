import logging
from typing import List, Dict, Any, Optional
from sentence_transformers import CrossEncoder
import numpy as np
import asyncio

# Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ù…Ø¯ÙŠØ± Ø§Ù„ÙƒØ§Ø´
from ..core.cache_manager import CacheManager 

logger = logging.getLogger(__name__)

class CrossEncoderRanker:
    """
    ÙŠØ³ØªØ®Ø¯Ù… Ù†Ù…ÙˆØ°Ø¬ Cross-Encoder (Ø§Ù„Ù…ØµÙ†Ù Ø§Ù„Ù…ØªÙ‚Ø¯Ù…) Ù„Ø¥Ø¹Ø§Ø¯Ø© ØªØ±ØªÙŠØ¨ 
    Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ø£ÙˆÙ„ÙŠØ© Ø§Ù„Ù…Ø³ØªØ±Ø¬Ø¹Ø© Ù…Ù† Ø§Ù„Ù€ semantic_retriever.
    Ù‡Ø°Ù‡ Ù‡ÙŠ "Ø§Ù„Ù…Ø±Ø­Ù„Ø© Ø§Ù„Ø«Ø§Ù†ÙŠØ©" Ù…Ù† Ø§Ù„Ø§Ø³ØªØ±Ø¬Ø§Ø¹ (Reranking) Ù„Ø¶Ù…Ø§Ù† Ø£Ø¹Ù„Ù‰ Ø¯Ù‚Ø©.
    """
    
    def __init__(
        self, 
        model_name: str = 'BAAI/bge-reranker-base', 
        top_k: int = 5,
        batch_size: int = 16,
        cache_manager: Optional[CacheManager] = None
    ):
        """
        ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ù…ØµÙ†Ù.
        
        Args:
            model_name: Ø§Ø³Ù… Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù€ Cross-Encoder Ù…Ù† HuggingFace.
                        (BAAI/bge-reranker-base Ù‡Ùˆ Ø®ÙŠØ§Ø± Ù…Ù…ØªØ§Ø² ÙˆØ®ÙÙŠÙ)
            top_k: Ø¹Ø¯Ø¯ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ© Ø§Ù„ØªÙŠ Ø³ÙŠØªÙ… Ø¥Ø±Ø¬Ø§Ø¹Ù‡Ø§ Ø¨Ø¹Ø¯ Ø¥Ø¹Ø§Ø¯Ø© Ø§Ù„ØªØ±ØªÙŠØ¨.
            batch_size: Ø­Ø¬Ù… Ø§Ù„Ø¯ÙØ¹Ø© Ù„Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© (Ù„Ø²ÙŠØ§Ø¯Ø© ÙƒÙØ§Ø¡Ø© Ø§Ù„Ù€ GPU/CPU).
            cache_manager: Ù…Ø¯ÙŠØ± Ø§Ù„ÙƒØ§Ø´ Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ù…ÙƒÙ„ÙØ© Ø­Ø³Ø§Ø¨ÙŠØ§Ù‹.
        """
        try:
            self.model = CrossEncoder(model_name)
            self.top_k = top_k
            self.batch_size = batch_size
            self.cache_manager = cache_manager
            logger.info(f"âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ CrossEncoderRanker Ø¨Ù†Ø¬Ø§Ø­ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù†Ù…ÙˆØ°Ø¬: {model_name}")
        except Exception as e:
            logger.error(f"âŒ ÙØ´Ù„ ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ CrossEncoder: {model_name}. Ø®Ø·Ø£: {e}")
            raise

    async def rerank_documents(self, query: str, candidates: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """
        Ø¥Ø¹Ø§Ø¯Ø© ØªØ±ØªÙŠØ¨ Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª Ø§Ù„Ù…Ø±Ø´Ø­Ø© (candidates) Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù… (query).
        """
        if not candidates:
            logger.debug("Ù„Ø§ ØªÙˆØ¬Ø¯ Ù…Ø³ØªÙ†Ø¯Ø§Øª Ù…Ø±Ø´Ø­Ø© Ù„Ø¥Ø¹Ø§Ø¯Ø© Ø§Ù„ØªØ±ØªÙŠØ¨.")
            return []
            
        # --- 1. Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„ÙƒØ§Ø´ ---
        # Ù…ÙØªØ§Ø­ Ø§Ù„ÙƒØ§Ø´ ÙŠØ¹ØªÙ…Ø¯ Ø¹Ù„Ù‰ Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù… ÙˆÙ…Ø­ØªÙˆÙ‰ Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª Ø§Ù„Ù…Ø±Ø´Ø­Ø©
        if self.cache_manager:
            try:
                candidate_ids = hash(tuple(doc['metadata'].get('chunk_id', doc['content']) for doc in candidates))
                cache_key = f"rerank:{query}:{candidate_ids}"
                cached_results = await self.cache_manager.get(cache_key)
                if cached_results:
                    logger.debug(f"ğŸ” ØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ù†ØªØ§Ø¦Ø¬ Ø¥Ø¹Ø§Ø¯Ø© Ø§Ù„ØªØ±ØªÙŠØ¨ ÙÙŠ Ø§Ù„ÙƒØ§Ø´ Ù„Ù€: {query[:50]}...")
                    return cached_results
            except Exception as e:
                logger.warning(f"âš ï¸ ÙØ´Ù„ Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ÙƒØ§Ø´ Ø¥Ø¹Ø§Ø¯Ø© Ø§Ù„ØªØ±ØªÙŠØ¨: {e}")

        # --- 2. Ø¥Ø¹Ø¯Ø§Ø¯ Ø£Ø²ÙˆØ§Ø¬ (Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…ØŒ Ø§Ù„Ù…Ø³ØªÙ†Ø¯) Ù„Ù„Ù†Ù…ÙˆØ°Ø¬ ---
        # (query, document_content)
        pairs = [(query, doc['content']) for doc in candidates]
        
        # --- 3. ØªØ´ØºÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ (Ø¹Ù…Ù„ÙŠØ© Ù…ÙƒÙ„ÙØ©) ---
        # Ù†Ø³ØªØ®Ø¯Ù… asyncio.to_thread Ù„ØªØ´ØºÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ (ÙˆÙ‡Ùˆ Ù…ØªØ²Ø§Ù…Ù† Sync) 
        # ÙÙŠ "Ø«Ø±ÙŠØ¯" Ù…Ù†ÙØµÙ„ Ù„Ù…Ù†Ø¹ ØªØ¬Ù…ÙŠØ¯ (Blocking) Ø§Ù„Ù€ Event Loop.
        try:
            logger.debug(f"ğŸš€ Ø¨Ø¯Ø¡ Ø¥Ø¹Ø§Ø¯Ø© ØªØ±ØªÙŠØ¨ {len(pairs)} Ù…Ø³ØªÙ†Ø¯ Ù„Ù€: {query[:50]}...")
            
            def _predict():
                # ØªØ´ØºÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù…Ø¹ batching Ù„Ø¥Ø¯Ø§Ø±Ø© Ø§Ù„Ø°Ø§ÙƒØ±Ø© ÙˆØ§Ù„Ø£Ø¯Ø§Ø¡
                return self.model.predict(
                    pairs, 
                    batch_size=self.batch_size, 
                    show_progress_bar=False  # (ÙŠÙ…ÙƒÙ† ØªÙØ¹ÙŠÙ„Ù‡Ø§ Ù„Ù„Ù€ debugging)
                )

            # ØªØ´ØºÙŠÙ„ Ø§Ù„Ø¯Ø§Ù„Ø© Ø§Ù„Ù…ØªØ²Ø§Ù…Ù†Ø© ÙÙŠ Ø«Ø±ÙŠØ¯ Ù…Ù†ÙØµÙ„
            scores = await asyncio.to_thread(_predict)
            
            # Ø§Ù„ØªØ£ÙƒØ¯ Ù…Ù† Ø£Ù† Ø§Ù„Ù…Ø®Ø±Ø¬Ø§Øª Ù‡ÙŠ numpy array
            if not isinstance(scores, np.ndarray):
                scores = np.array(scores)

            logger.debug(f"ğŸ“Š ØªÙ… Ø­Ø³Ø§Ø¨ Ø¯Ø±Ø¬Ø§Øª Ø¥Ø¹Ø§Ø¯Ø© Ø§Ù„ØªØ±ØªÙŠØ¨ Ø¨Ù†Ø¬Ø§Ø­.")

        except Exception as e:
            logger.error(f"âŒ Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ ØªØ´ØºÙŠÙ„ CrossEncoder.predict: {e}")
            # ÙÙŠ Ø­Ø§Ù„Ø© Ø§Ù„ÙØ´Ù„ØŒ Ù†Ø±Ø¬Ø¹ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ø£ØµÙ„ÙŠØ© Ø¨ØªØ±ØªÙŠØ¨Ù‡Ø§
            return candidates[:self.top_k]

        # --- 4. Ø¯Ù…Ø¬ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ ÙˆÙØ±Ø²Ù‡Ø§ ---
        ranked_results = []
        for i, doc in enumerate(candidates):
            new_score = float(scores[i])
            
            # Ø¥Ø¶Ø§ÙØ©/ØªØ­Ø¯ÙŠØ« Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ÙˆØµÙÙŠØ©
            doc['rerank_score'] = new_score
            # (Ø§Ø®ØªÙŠØ§Ø±ÙŠ: Ø§Ù„Ø§Ø­ØªÙØ§Ø¸ Ø¨Ø§Ù„Ø¯Ø±Ø¬Ø© Ø§Ù„Ø£ØµÙ„ÙŠØ© Ù„Ù„Ù…Ù‚Ø§Ø±Ù†Ø©)
            if 'similarity' in doc:
                doc['metadata']['original_similarity'] = doc.get('similarity')
            
            # Ø§Ø³ØªØ¨Ø¯Ø§Ù„ Ø¯Ø±Ø¬Ø© Ø§Ù„ØªØ´Ø§Ø¨Ù‡ Ø§Ù„Ù‚Ø¯ÙŠÙ…Ø© Ø¨Ø§Ù„Ø¯Ø±Ø¬Ø© Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø© Ø§Ù„Ø£ÙƒØ«Ø± Ø¯Ù‚Ø©
            doc['similarity'] = new_score 
            ranked_results.append(doc)

        # Ø§Ù„ÙØ±Ø² Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ø¯Ø±Ø¬Ø© Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø© (Ø§Ù„Ø£Ø¹Ù„Ù‰ Ø£ÙØ¶Ù„)
        sorted_results = sorted(ranked_results, key=lambda x: x['rerank_score'], reverse=True)
        
        # Ø§Ø®ØªÙŠØ§Ø± Ø£ÙØ¶Ù„ K Ù†ØªØ§Ø¦Ø¬ ÙÙ‚Ø·
        final_results = sorted_results[:self.top_k]

        # --- 5. ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù†ØªÙŠØ¬Ø© ÙÙŠ Ø§Ù„ÙƒØ§Ø´ ---
        if self.cache_manager:
            try:
                # (cache_key ØªÙ… Ø­Ø³Ø§Ø¨Ù‡ ÙÙŠ Ø§Ù„Ø®Ø·ÙˆØ© 1)
                await self.cache_manager.set(cache_key, final_results, ttl=3600) # ØªØ®Ø²ÙŠÙ† Ù„Ù…Ø¯Ø© Ø³Ø§Ø¹Ø©
            except Exception as e:
                logger.warning(f"âš ï¸ ÙØ´Ù„ ØªØ®Ø²ÙŠÙ† Ù†ØªØ§Ø¦Ø¬ Ø¥Ø¹Ø§Ø¯Ø© Ø§Ù„ØªØ±ØªÙŠØ¨ ÙÙŠ Ø§Ù„ÙƒØ§Ø´: {e}")

        return final_results

