# backend/app/court_simulation/performance_analyzer.py
from datetime import datetime
from typing import Dict, List, Any
import json

class PerformanceAnalyzer:
    """
    Ù…Ø­Ù„Ù„ Ø£Ø¯Ø§Ø¡ Ù…ØªÙ‚Ø¯Ù… Ù„ØªÙ‚ÙŠÙŠÙ… Ø£Ø¯Ø§Ø¡ Ø§Ù„Ù…Ø­Ø§Ù…ÙŠ ÙÙŠ Ø§Ù„Ù…Ø­Ø§ÙƒØ§Ø©
    """
    
    def __init__(self):
        self.metrics_history = []
        
    async def analyze_response_quality(self, user_response: str, context: Dict) -> Dict[str, Any]:
        """ØªØ­Ù„ÙŠÙ„ Ø¬ÙˆØ¯Ø© Ø±Ø¯ Ø§Ù„Ù…Ø­Ø§Ù…ÙŠ"""
        
        analysis = {
            "legal_accuracy": self._evaluate_legal_accuracy(user_response, context),
            "argument_strength": self._evaluate_argument_strength(user_response),
            "clarity_and_structure": self._evaluate_clarity(user_response),
            "evidence_usage": self._evaluate_evidence_usage(user_response, context),
            "response_time": context.get("response_time", 0)
        }
        
        # Ø­Ø³Ø§Ø¨ Ø§Ù„Ù†ØªÙŠØ¬Ø© Ø§Ù„Ø¥Ø¬Ù…Ø§Ù„ÙŠØ©
        analysis["overall_score"] = self._calculate_overall_score(analysis)
        analysis["feedback"] = self._generate_feedback(analysis, user_response)
        
        return analysis
    
    def _evaluate_legal_accuracy(self, response: str, context: Dict) -> float:
        """ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ø¯Ù‚Ø© Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ© (0-100)"""
        # Ù…Ø¤Ù‚ØªØ§Ù‹ - Ø¥Ø±Ø¬Ø§Ø¹ Ù‚ÙŠÙ…Ø© Ø§ÙØªØ±Ø§Ø¶ÙŠØ©
        legal_terms = ["Ø§Ù„Ù…Ø§Ø¯Ø©", "Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†", "Ø§Ù„Ø³Ø§Ø¨Ù‚Ø©", "Ø§Ù„Ø­ÙƒÙ…", "Ø§Ù„Ø¯ÙØ§Ø¹", "Ø§Ù„Ø¥Ø«Ø¨Ø§Øª"]
        found_terms = sum(1 for term in legal_terms if term in response)
        return min(100, (found_terms / len(legal_terms)) * 100)
    
    def _evaluate_argument_strength(self, response: str) -> float:
        """ØªÙ‚ÙŠÙŠÙ… Ù‚ÙˆØ© Ø§Ù„Ø­Ø¬Ø© (0-100)"""
        # ØªØ­Ù„ÙŠÙ„ Ù‡ÙŠÙƒÙ„ Ø§Ù„Ø­Ø¬Ø©
        argument_indicators = ["Ù„Ø°Ù„Ùƒ", "ÙˆØ¨Ø§Ù„ØªØ§Ù„ÙŠ", "Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰", "Ù…Ù…Ø§ ÙŠØ«Ø¨Øª", "ÙŠØ¯Ù„ Ø¹Ù„Ù‰"]
        indicators_count = sum(1 for indicator in argument_indicators if indicator in response)
        
        # Ø­Ø³Ø§Ø¨ Ø§Ù„Ø·ÙˆÙ„ Ø§Ù„Ù†Ø³Ø¨ÙŠ
        word_count = len(response.split())
        length_score = min(100, (word_count / 50) * 100)  # 50 ÙƒÙ„Ù…Ø© Ù‡Ø¯Ù
        
        return (indicators_count * 20 + length_score * 0.3)
    
    def _evaluate_clarity(self, response: str) -> float:
        """ØªÙ‚ÙŠÙŠÙ… Ø§Ù„ÙˆØ¶ÙˆØ­ ÙˆØ§Ù„ØªÙ†Ø¸ÙŠÙ… (0-100)"""
        # Ù…Ø¤Ø´Ø±Ø§Øª Ø§Ù„ØªÙ†Ø¸ÙŠÙ…
        structure_indicators = ["Ø£ÙˆÙ„Ø§Ù‹", "Ø«Ø§Ù†ÙŠØ§Ù‹", "Ø®ØªØ§Ù…Ø§Ù‹", "Ù…Ù† Ù†Ø§Ø­ÙŠØ©", "Ù…Ù† Ù†Ø§Ø­ÙŠØ© Ø£Ø®Ø±Ù‰"]
        structure_score = sum(1 for indicator in structure_indicators if indicator in response) * 15
        
        # Ù†Ù‚Ø§Ø· Ø§Ù„ÙˆØ¶ÙˆØ­
        clarity_score = min(100, structure_score + 40)  # Ø¯Ø±Ø¬Ø© Ø£Ø³Ø§Ø³ÙŠØ© + Ù‡ÙŠÙƒÙ„
        
        return clarity_score
    
    def _evaluate_evidence_usage(self, response: str, context: Dict) -> float:
        """ØªÙ‚ÙŠÙŠÙ… Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø£Ø¯Ù„Ø© (0-100)"""
        evidence_terms = ["Ø§Ù„Ø¯Ù„ÙŠÙ„", "Ø§Ù„ÙˆØ«ÙŠÙ‚Ø©", "Ø§Ù„Ø¥Ø«Ø¨Ø§Øª", "Ø§Ù„Ø¨ÙŠÙ†Ø©", "Ø§Ù„Ø´Ù‡Ø§Ø¯Ø©"]
        evidence_count = sum(1 for term in evidence_terms if term in response)
        
        # Ø¥Ø°Ø§ ÙƒØ§Ù† Ù‡Ù†Ø§Ùƒ Ø£Ø¯Ù„Ø© ÙÙŠ Ø§Ù„Ø³ÙŠØ§Ù‚ ÙˆØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡Ø§
        available_evidence = context.get("available_evidence", [])
        if available_evidence:
            used_evidence = sum(1 for evidence in available_evidence if evidence in response)
            evidence_score = (used_evidence / len(available_evidence)) * 50
        
        return min(100, (evidence_count * 20) + evidence_score)
    
    def _calculate_overall_score(self, analysis: Dict) -> float:
        """Ø­Ø³Ø§Ø¨ Ø§Ù„Ù†ØªÙŠØ¬Ø© Ø§Ù„Ø¥Ø¬Ù…Ø§Ù„ÙŠØ© Ù…Ø¹ Ø£ÙˆØ²Ø§Ù† Ù…Ø®ØªÙ„ÙØ©"""
        weights = {
            "legal_accuracy": 0.35,
            "argument_strength": 0.25,
            "clarity_and_structure": 0.20,
            "evidence_usage": 0.20
        }
        
        overall_score = 0
        for metric, weight in weights.items():
            overall_score += analysis[metric] * weight
        
        return round(overall_score, 2)
    
    def _generate_feedback(self, analysis: Dict, response: str) -> List[str]:
        """ØªÙˆÙ„ÙŠØ¯ ØªØ¹Ù„ÙŠÙ‚Ø§Øª Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„ØªØ­Ù„ÙŠÙ„"""
        feedback = []
        
        if analysis["legal_accuracy"] < 60:
            feedback.append("ğŸ’¡ ØªØ­ØªØ§Ø¬ Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù…ØµØ·Ù„Ø­Ø§Øª Ù‚Ø§Ù†ÙˆÙ†ÙŠØ© Ø£ÙƒØ«Ø± Ø¯Ù‚Ø©")
        
        if analysis["argument_strength"] < 50:
            feedback.append("ğŸ’¡ Ø­Ø¬Ø¬Ùƒ ØªØ­ØªØ§Ø¬ Ù„Ù…Ø²ÙŠØ¯ Ù…Ù† Ø§Ù„Ù‚ÙˆØ© ÙˆØ§Ù„Ù…Ù†Ø·Ù‚")
            
        if analysis["clarity_and_structure"] < 60:
            feedback.append("ğŸ’¡ Ø±ØªØ¨ Ø£ÙÙƒØ§Ø±Ùƒ Ø¨Ø´ÙƒÙ„ Ø£ÙØ¶Ù„ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø¹Ù†Ø§ØµØ± ØªØ±Ù‚ÙŠÙ…")
            
        if analysis["evidence_usage"] < 40:
            feedback.append("ğŸ’¡ Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø£Ø¯Ù„Ø© Ø§Ù„Ù…ØªØ§Ø­Ø© Ø¨Ø´ÙƒÙ„ Ø£ÙƒØ«Ø± ÙØ¹Ø§Ù„ÙŠØ©")
            
        if analysis["overall_score"] >= 80:
            feedback.append("ğŸ‰ Ø£Ø¯Ø§Ø¡ Ù…Ù…ØªØ§Ø²! Ø§Ø³ØªÙ…Ø± ÙÙŠ Ù‡Ø°Ø§ Ø§Ù„Ù…Ø³ØªÙˆÙ‰")
        elif analysis["overall_score"] >= 60:
            feedback.append("ğŸ‘ Ø£Ø¯Ø§Ø¡ Ø¬ÙŠØ¯ØŒ Ù…Ø¹ Ø¥Ù…ÙƒØ§Ù†ÙŠØ© Ø§Ù„ØªØ­Ø³ÙŠÙ†")
        else:
            feedback.append("ğŸ“š ØªØ­ØªØ§Ø¬ Ù„Ù…Ø²ÙŠØ¯ Ù…Ù† Ø§Ù„ØªØ¯Ø±ÙŠØ¨ ÙˆØ§Ù„Ù…Ù…Ø§Ø±Ø³Ø©")
            
        return feedback
    
    async def track_session_performance(self, session_id: str, responses: List[Dict]):
        """ØªØªØ¨Ø¹ Ø£Ø¯Ø§Ø¡ Ø§Ù„Ø¬Ù„Ø³Ø© ÙƒØ§Ù…Ù„Ø©"""
        session_metrics = {
            "session_id": session_id,
            "start_time": datetime.now(),
            "total_responses": len(responses),
            "average_score": 0,
            "improvement_trend": False,
            "weak_areas": []
        }
        
        if responses:
            scores = [resp.get("analysis", {}).get("overall_score", 0) for resp in responses]
            session_metrics["average_score"] = sum(scores) / len(scores)
            
            # ØªØ­Ù„ÙŠÙ„ Ø§ØªØ¬Ø§Ù‡ Ø§Ù„ØªØ­Ø³Ù†
            if len(scores) > 1:
                session_metrics["improvement_trend"] = scores[-1] > scores[0]
            
            # ØªØ­Ø¯ÙŠØ¯ Ù†Ù‚Ø§Ø· Ø§Ù„Ø¶Ø¹Ù
            weak_areas = self._identify_weak_areas(responses)
            session_metrics["weak_areas"] = weak_areas
        
        self.metrics_history.append(session_metrics)
        return session_metrics
    
    def _identify_weak_areas(self, responses: List[Dict]) -> List[str]:
        """ØªØ­Ø¯ÙŠØ¯ Ù…Ø¬Ø§Ù„Ø§Øª Ø§Ù„Ø¶Ø¹Ù Ø§Ù„Ù…ØªÙƒØ±Ø±Ø©"""
        area_scores = {
            "legal_accuracy": [],
            "argument_strength": [],
            "clarity_and_structure": [],
            "evidence_usage": []
        }
        
        for response in responses:
            analysis = response.get("analysis", {})
            for area in area_scores.keys():
                if area in analysis:
                    area_scores[area].append(analysis[area])
        
        weak_areas = []
        for area, scores in area_scores.items():
            if scores and sum(scores) / len(scores) < 60:
                weak_areas.append(area)
                
        return weak_areas
    
    def get_performance_report(self, session_id: str) -> Dict:
        """Ø¥Ø¹Ø¯Ø§Ø¯ ØªÙ‚Ø±ÙŠØ± Ø£Ø¯Ø§Ø¡ Ù…ÙØµÙ„"""
        session_data = next((s for s in self.metrics_history if s["session_id"] == session_id), None)
        
        if not session_data:
            return {"error": "Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø¬Ù„Ø³Ø©"}
        
        report = {
            "session_summary": session_data,
            "recommendations": self._generate_recommendations(session_data),
            "comparison_to_previous": self._compare_with_previous(session_id),
            "next_steps": self._suggest_next_steps(session_data)
        }
        
        return report
    
    def _generate_recommendations(self, session_data: Dict) -> List[str]:
        """ØªÙˆÙ„ÙŠØ¯ ØªÙˆØµÙŠØ§Øª Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ù†Ù‚Ø§Ø· Ø§Ù„Ø¶Ø¹Ù"""
        recommendations = []
        weak_areas = session_data.get("weak_areas", [])
        
        area_recommendations = {
            "legal_accuracy": ["Ø§Ø¯Ø±Ø³ Ø§Ù„ØªØ´Ø±ÙŠØ¹Ø§Øª Ø°Ø§Øª Ø§Ù„ØµÙ„Ø©", "Ø±Ø§Ø¬Ø¹ Ø§Ù„Ø³ÙˆØ§Ø¨Ù‚ Ø§Ù„Ù‚Ø¶Ø§Ø¦ÙŠØ©"],
            "argument_strength": ["ØªØ¯Ø±Ø¨ Ø¹Ù„Ù‰ Ø¨Ù†Ø§Ø¡ Ø§Ù„Ø­Ø¬Ø¬ Ø§Ù„Ù…Ù†Ø·Ù‚ÙŠØ©", "ØªØ¹Ù„Ù… ØªÙ‚Ù†ÙŠØ§Øª Ø§Ù„Ø¥Ù‚Ù†Ø§Ø¹"],
            "clarity_and_structure": ["Ø±ØªØ¨ Ø£ÙÙƒØ§Ø±Ùƒ Ù‚Ø¨Ù„ Ø§Ù„ØªØ­Ø¯Ø«", "Ø§Ø³ØªØ®Ø¯Ù… Ø¹Ù†Ø§ØµØ± Ø§Ù„ØªØ±Ù‚ÙŠÙ…"],
            "evidence_usage": ["ØªØ¹Ù„Ù… ÙƒÙŠÙÙŠØ© Ø§Ù„Ø§Ø³ØªØ´Ù‡Ø§Ø¯ Ø¨Ø§Ù„Ø£Ø¯Ù„Ø©", "Ø±Ø§Ø¬Ø¹ Ù‚ÙˆØ§Ø¹Ø¯ Ø§Ù„Ø¥Ø«Ø¨Ø§Øª"]
        }
        
        for area in weak_areas:
            if area in area_recommendations:
                recommendations.extend(area_recommendations[area])
                
        return recommendations
    
    def _compare_with_previous(self, current_session_id: str) -> Dict:
        """Ù…Ù‚Ø§Ø±Ù†Ø© Ù…Ø¹ Ø§Ù„Ø¬Ù„Ø³Ø§Øª Ø§Ù„Ø³Ø§Ø¨Ù‚Ø©"""
        current_index = next(i for i, s in enumerate(self.metrics_history) if s["session_id"] == current_session_id)
        
        if current_index > 0:
            previous_session = self.metrics_history[current_index - 1]
            current_session = self.metrics_history[current_index]
            
            return {
                "score_change": current_session["average_score"] - previous_session["average_score"],
                "improvement": current_session["average_score"] > previous_session["average_score"],
                "message": f"ØªØ­Ø³Ù† Ø¨Ù…Ù‚Ø¯Ø§Ø± {current_session['average_score'] - previous_session['average_score']:.1f} Ù†Ù‚Ø·Ø©" 
                if current_session["average_score"] > previous_session["average_score"] 
                else "Ø§Ù†Ø®ÙØ§Ø¶ ÙÙŠ Ø§Ù„Ø£Ø¯Ø§Ø¡"
            }
        
        return {"message": "Ù‡Ø°Ù‡ Ø£ÙˆÙ„ Ø¬Ù„Ø³Ø© Ù„Ùƒ"}
    
    def _suggest_next_steps(self, session_data: Dict) -> List[str]:
        """Ø§Ù‚ØªØ±Ø§Ø­ Ø§Ù„Ø®Ø·ÙˆØ§Øª Ø§Ù„ØªØ§Ù„ÙŠØ© Ù„Ù„ØªØ­Ø³ÙŠÙ†"""
        score = session_data.get("average_score", 0)
        
        if score >= 80:
            return ["Ø¬Ø±Ø¨ Ù‚Ø¶Ø§ÙŠØ§ Ø£ÙƒØ«Ø± ØªØ¹Ù‚ÙŠØ¯Ø§Ù‹", "ØªØ¯Ø±Ø¨ Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø­Ø§ÙƒÙ…Ø§Øª Ø§Ù„Ù…ØªÙ‚Ø¯Ù…Ø©"]
        elif score >= 60:
            return ["Ø±Ø§Ø¬Ø¹ Ù†Ù‚Ø§Ø· Ø§Ù„Ø¶Ø¹Ù", "Ø¬Ø±Ø¨ Ù†ÙØ³ Ø§Ù„Ù†ÙˆØ¹ Ù…Ù† Ø§Ù„Ù‚Ø¶Ø§ÙŠØ§ Ù…Ø±Ø© Ø£Ø®Ø±Ù‰"]
        else:
            return ["Ø§Ø¨Ø¯Ø£ Ø¨Ù‚Ø¶Ø§ÙŠØ§ Ø¨Ø³ÙŠØ·Ø©", "Ø±Ø§Ø¬Ø¹ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ§Øª Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ©", "ØªØ¯Ø±Ø¨ Ø¹Ù„Ù‰ Ø¨Ù†Ø§Ø¡ Ø§Ù„Ø­Ø¬Ø¬"]